# Lesson outline
- [L09a: Giant Scale Services](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L09_Internet%20Computing.md#l09a-giant-scale-services)
- [L09b: MapReduce](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L09_Internet%20Computing.md#l09b-mapreduce)
- [L09c: Content Delivery Networks](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L09_Internet%20Computing.md#l09c-content-delivery-networks)

# L09a: Giant Scale Services
<h2>1. Giant Scale Services Introduction</h2>
<ul>
  <li>What are the systems issues in managing large data centers? How do you program big data applications, such as search engines to run on massively large clusters? How do you store and disseminate content on the web in a scalable manner? These are the kinds of the questions we will be addressing in this module of the course. This is what we mean when we use the term internet scale computing. A comforting factor for you is that you already know the answer to these questions. We've learned through the previous lessons, most of the techniques needed in constructing distributed services. What we're going to see in the upcoming lesson is a hardening of the distributed systems issues to handle scale on the order of thousands of processes and failures. It's not a question of if. It's a question of when a failure's going to happen.</li> 
</ul>

<h2>2. Giant Scale Services</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/1.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/2.JPG?raw=true" alt="drawing" width="500"/>
</p>

<h2>3. Tablet Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/3.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>In this module, we will cover three topics. The first is systems issues in providing giant scale services on the internet. And the second topic is what exactly is a programming model that is used for the applications that are delivering the service that you're expecting on an every day basis, so-called Big Data applications. And the third topic is content distribution networks that are used for the dissemination of the content when you access giant scale services on the internet. So in this lesson, we are focusing on what are the issues in giant scale services from the perspective of the system.</li> 
</ul>

<h2>4. Generic Service Model of Giant Scale Services</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/4.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Imagine you're going on the Internet and reaching a web portal, such as gmail, let's say, for accessing your email. In fact, while you're doing this, a million other clients are doing exactly the same thing trying to reach the web portal through the IP network. The service that you're trying to reach may be architected to direct your particular request to a specific site. And the architecture within that site may look like this, a whole bunch of servers, maybe on the order of thousands or even 10,000 within a cold dark room, interconnected to one another through perhaps a high bandwidth communication backplane. And also all of the servers are connected to data stores for processing requests that may be coming in. And the servers may optionally use a backplane that allows them to talk to one another for servicing any particular request. Any of the servers may be able to handle an incoming client request.</li> 
  <li>Now in the generic picture that I'm showing you here, the load manager that is sitting in between these servers, and the IP network redirects an incoming client's request to one the servers. The chosen server may than process the query and answer the client. The role of the load manager is to balance the client traffic among all the servers. Basically, the load manager has to make sure that no particular server is overloaded. One of the characteristics of most of the giant scale services is that the these client requests that are coming in are all independent of one another. Sometimes, this also called embarrassingly parallel, meaning that all of these client requests can be handled in parallel as long as it is enough server capacity to meet all the incoming requests. So the rule of the load manager is to balance the client traffic and redirect them to the server so that no particular server is overloaded and all of the servers get equally utilized in servicing incoming requests.</li> 
  <li>The other responsibility of the load manager is to hide partial failures. Because when we're talking about the scale of giant scale services, and the kind of computational power that resides in the data center for servicing these giant scale services, they're humongous, they're 10,000 nodes. And when you have so many nodes, compute nodes and data nodes and so on, it's not a question of if something will fail, but is a question of when something is going to fail. So the load manager has to make sure that it is observing the state of the servers and ensuring that the incoming client requests are shielded from any such failures that may happen internally within a particular site.</li> 
</ul>

<h2>5. Clusters as Workhorses</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/5.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Compuptational clusters are the workhorses of giant scale services, any modern data center employs on the order of thousands and thousands of computational nodes connected by high-performance high-speed networks. It is such computational clusters, which are serving as the workhorses for giant scale services today. And this figure reproduced from Eric A. Brewer's paper circa 2,000 shows the number of nodes in a data center, maybe on the order of 1,000. But scale it up by ten or 100 in terms of today's computation capacity that you might find in a typical data center. And the kind of volume of queries that are going to be handled by giant scale services today.</li> 
  <li>Each node in the cluster may itself be an SMP. There are serveral virtues for structuring the computational resources inside a data center as a cluster of machines connected by high performance backplanes. The advantages include absolute scalability. You can keep adding more resources without worrying about re-architecting the internals of the data center. Cost and performance is another important issue and that is by having computational clusters at every node is identical to others, is that as a system administrator we can control both the cost and the performance of running a data center like that. Because of the independent components you can easily mix and match generational changes in the hardware, and the results are incremental scaleability you add more resources you get more in terms of performance because, as I told you earlier, most of the queries that come into giant-scale services tend to be embarrassingly parallel. So the more resources you've got, the more queries you can handle. There is incremental scalability when you're using clusters because you can add more nodes to the cluster, increasing the performance. Or, if the volume of request come down, you can scale back. That's the advantage of using computational clusters as the workhorses for data centers that are serving giant scale services today.</li> 
</ul>

<h2>6. Load Management Choices</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/6.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>As a refresher, I'm showing you the seven layer OSI reference model of the protocol stack. Now, load management, as I mentioned, for dealing with client requests, can be done at any of the levels of the seven layer OSI model from the network layer and up. And higher the layer, you have more functionality that you can associate with the load manager in the terms of how it deals with the server failures, how it deals with directing incoming client requests to the different servers, all of these are possible. The higher the layer at which you construct the load manager.
</li> 
</ul>

<h2>7. Load Management at Network Level</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/7.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>The load manager may be operating at the network level, that is, at the network layer level of the seven layer OSI reference model. In this case, the load manager is simply a round robin domain name server. What it does is, when client request comes in, they all have the same domain name in order to come to this particular server. If you say gmail.com, then depending on how the server is architected, the gmail request coming from particular client may go to a particular site. And when it goes to that site, the domain name server, which is acting as a load manager, it is going to direct the incoming client request to one of the servers. So it is giving different IP addresses. Even though the domain name a client is coming in is exactly the same, the round robin DNS server assigns different IP addresses corresponding to different servers to the incoming client request. And that way these clients are being redirected to different servers, and because the load manager is doing it at the level of individual server addresses, you get very good load balance. And the the inherent assumption in this model, is that all the servers are identical, so an incoming request can be sent to any one of these servers, and perhaps the model is also that the data is fully replicated. So that if an incoming request goes to this server, it has access to all the data. If it goes to this server, it has access to all the data to satisfy the incoming client request.</li> 
  <li>The pro is good load balance because the Round Robin DNS scheduler can choose the least loaded server to redirect an incoming client to that particular server.</li> 
  <li>The con is, it cannot hide down server nodes, because it is assigning IP addresses of the server to the incoming request, the load manager is not able to hide down server nodes from the external world. </li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/8.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So now we move the load manager up in the OSI reference model, up to the transport level or even higher. The load manager could be layer four switches. That is transport level switches or even higher level switches. In fact, the layer four switches may be architected as switch pairs so that there is hot failover from one fail switch to another fail switch at the load balancer level. When you do this architectecting the load balancer at the transport or higher levels. It gives you the opportunity to dynamically isolate down server nodes from the external world, and it is also possible with this architecture of load manager to have service specific front end nodes. For example, instead of dealing with arbitrary client requests, we're actually dealing with requests based on the kind of requests coming in from a particular client. For instance, this could be a Gmail client. This could be a Google search client. This could be for a photo server like Picasa. So those are different functionalities that can be implemented at the frontend of this load manager to look at the service, the particular program, that is actually triggering a client request coming in to this particular site, and make decisions as to who should serve that particular request. So in other words, we're building more intelligence into the load manager. So that it can know the semantics of the client server communication that is going to happen based on the type of request that is coming in.</li> 
  <li>It is also possible with this structure of having the load management at higher levels of the OSI reference model to co-opt the client devices in the load management. For example, it's possible that the load manager may even know the device characteristics that the client is using, in order to come into the site. For example, if it's a smart phone, it might take certain actions commensurate with the device that is coming in, in terms of structuring the client-server interactions. Now, the data itself may be partitioned among all the servers. For example, let's say that you are doing a web search and it is a web search that came in as a clients request. And, if it is redirected to a particular server, this server may not have access to all the data that it needs. In order to do that, Google search that you came in with. In that case, the server may communicate using the backplane with other servers to get the data that it needs to serve the queries that is coming in, so that there is complete coverage of whatever the intended query needs to get as a result. Of course, if the data is partitioned, if a server that is holding a portion of the data is down, then there'll be data loss for the query. So for your specific search, the server that is handling your search may not be able to give you complete result for the search, because a portion of the data is down due to the partitioning of the data among the different servers at this site. Now, this is where replicating the servers for redundancy helps. So even though you may partition it, you may replicate partitions so that it is possible that even if one server is down, another server that has a replica of the same partition may be able to provide the data that is needed in order to satisfy a particular query.</li> 
</ul>


<h2>8. DQ Principle</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/9.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Thus far, I've given you a general idea of how an individual site that hosts computation resources and handles a whole bunch of incoming client requests may handle these incoming client requests in terms of load management. This brings us to the DQ principle.</li> 
  <li>The server has all the data that is required for dealing with incoming client queries. So this the full data set, or Df if the full data set, that is required for handling any incoming queries to the server. Now clients come in with their queries and we will call Q0 as the offered load to the server, meaning this is the amount of requests that are hitting the server per unit time. Even though the offered load is still Q0, maybe the server could only manage to serve a portion of this incoming requests. And that portion of the incoming requests that is completed by the server is Qc. These are the completed requests. We define the yield Q as the ratio: Qc over Qo. That is the ratio of completed requests to the offered load. So, Q is going to be a fraction between 0 and 1. Ideally you want it to be one so that all the client requests are serviced, but if the server is not able to deal with the offered load entirely, then the yield is going to be less than one.</li> 
  <li>Now each query that is coming into the server May require processing all of the data that's available in the server to answer this particular query. For instance, if there's a Google search and the search requires looking at all the corpus of data that the server has to answer the query, then you want to look at the full data set Df. However, it may be that because of either failures of some of the data servers or the load on the server, it is only able to process the query using a portion of the total data set, Dv. Dv is the available data. That is used in processing the query, in that case the harvest D is defined as the ratio Dv over Df. That is harvest is defined as the ratio of the available data to the full corpus of data. Again, this is going to be a ratio between 0 and 1. Ideally, you want to make sure that the harvest is one, meaning that incoming requests is completely served with all the data that it wants. Depending on the service I can give you different examples. So for instance if the incoming request is a search, you want to look at the whole corpus of data that is available to you to answer that search. But if you are not able to do that and the server is only using a portion of the full data in the harvest in terms of the quality of the results that is being given out to a particular client is less than one.
</li> 
</ul>

<h2>9. DQ Principle (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/10.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The product DQ, that is the data server query and the rate of query that is coming into the server. This DQ represents some sort of system limit for a given server capacity the product DQ is a constant. Said differently for a different system capacity, we can increase the number of clients that we are serving if we reduce the amount of data that we're using to process the incoming queries. That is, we're increasing Q by decreasing the harvest or D. Or we can do the opposite and that is, we can entertain a smaller set of queries. That is the yield will come down but we will be able to give the complete data that is needed for serving that query. An example of would be, if I am a client that is accessing a server for my mail, I would want to get all my mail, not part of the mail. So in that case, I will be happy if the server gives me a harvest of one, even it means that not all the incoming clients maybe served by the server due to the capacity limitation of the server at any point of time.</li> 
  <li>So as a system administrator, we have some choices to play with. The important thing is that in giant's scale services. The performance is limited by network and not by the I/O capacity. We can have as much I/O capacity as we want by having more hardware resources here, but what we are bound by is the network capacity. So if we increase the capacity of a server, we have a choice as a system administrator. We can either increase the harvest, keeping the yield the same, or we can increase the number of clients we are serving, that is, we can increase the yield keeping D a constant. That's the knob that a system administrator can play with in terms of dealing with capacity increases at the server.</li> 
  <li>Similarly, if I am a system administrator, and some nodes in the server fail, then again we have a choice of either decreasing the harvest or decreasing the yield in order to deal with reduced DQ value. It all depends on how the server itself is architected, if the failures can be hidden through replication and so on. But the important message I want to convey to you is that this DQ represents a system constant, so far as the server is concerned, in terms of the capacity. Given a particular capacity of the server, DQ is fixed. So as a system administrator you have a choice of either sacrificing yield for harvest or harvest for yield. You cannot increase both the harvest and yield without increasing the server capacity.</li> 
  <li>At traditional measure that has been used in servers, is the notion of IOOPS, or I/O operations per second, and these are meaningful in database kinds of applications where the applications are disbound, but the giant scale applications are network bound, and for network bound applications this manager DQ is much more intuitive as to what is going on in terms of managing the incoming load to the server and managing the corpus of data that the server has to look at in order to satisfy incoming queries.</li> 
  <li>Another metric that system administrators have often used is what is called uptime. You may have heard of terminologies like mean-time-between-failure(MTBF) and mean-time-to-repair(MTTR). Mean-time-between-failure is what is the expected time between two successive failures inside a data center. And similarly mean-time-to-repair is if a failure is detected, how much time does it take to bring up a failed server that need time to repair. And the uptime is defined as the ratio of the difference between mean-time-between-failure(MTBF) and mean-time-to-repair (MTTR). Uptime is equal to MTBF minus MTTR divided my MTBF. That is uptime. And you want the the uptime to be close to one. And usually uptime is measured in nines. Meaning point nine, nine, nine, nine. Five nines or six nines and so on. So you want the uptime to be as close to one as possible for giant scale services. But again, this uptime as a metric is not very satisfying because if there are no queries giving the time that is needed for repairing, that is, during the MTT par, the mean-time-to-repair, if no queries come into the server, then you haven't made anybody unhappy. In that sense, the uptime is not a very intuitive measure of how well a server is performing. It is better to look at yield as the output metric. And the harvest, again, is the output metric for say how well a server is able to deal with the dynamism and scale of requesting handled by a particular giant-scale service. We'll see that this DQ principle is very powerful in advising the system administrator on how to architect the system. How much to replicate? How much to partition in terms of the data set that the server is handling? How to deal with failures? How to gracefully degrade the servers when the volume of incoming traffic increases beyond a server capacity? All of these are questions that can be handled very elegantly using the DQ principle. And the key underlying assumption in the DQ principle is that the giant scaled services are network bound, and not I/O bound.
</li> 
</ul>

<h2>10. Replication vs Partitioning</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/11.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>In architecting the data store and the computational resources that back the data store at a server, the system administrator has a choice of either replicating or partitioning the data. If you replicating the data what that means is, that every data server has the full corpus of data that is needed to serve a particular request. Think of it as say, gmail. If it is gmail, then if a gmail request gets sent to anyone of these servers. They can handle the request that comes in because they have the full corpus of data to deal with a incoming request. We mentioned that failures are inevitable in giant scale services because of the number of computation elements that live within a data center. If failures occur, what happens when we have replicated data? What they means is that the harvest is going to be unchanged because if some data repository is failed. Because it is replicated, we can redirect a client's request to another live server that has full access to the later repository and therefore the harvest that you're going to get with replication in 100%. It is unaffected. On the other hand because the server capacity has come down due to these failures, the yield is going to come down. In other words, with replication. The service is unavailable for some users for some amount of time. But all the users that are able to get the service, get complete harvest in term of the fidelity for the query that is returned and answered by the server.</li> 
  <li>The other alternative of course for architecting the internal data depository of the server is to partition the data. So if you partition the data, and lets say that There are end partitions of the full corpus of data. And let's say that some of the service fail, then some portion of the corpus of data becomes unavailable. If this happens, what it means is that the harvest is going to be down because some portion of the data corpus is unavailable, and therefore the harvest is going to come down, but it is possible to keep Q unchanged, so long as the computation capacity is unchanged. Then we can serve the same number of user community. It's just that the fidelity of the result may not be as good as when all of the data corpuses available. For example, if this is a server that is serving search results and if some portion of the data repository is unavailable. In that case, each query that comes in will get service, but it is going to get a subset of the query results for the search that they are doing.</li> 
  <li>So the important message that I want to convey through these pictures is that DQ is independent of whether we are replicating or we are partitioning the data. Given a certain server capacity, The DQs are constant, because we are assuming that this is cheap. This space is cheap and therefore processing incoming requests are not disk bound, but it is network bound. The only exception is if the queries Result in significant in right traffic to the disk. Now this is rare in giant scale services. But if that happens, then replication may require more DQ than partitioning. But as a first order of approximation, so long as we assume that giant scale services are serving client requests which are network bound, The DQ is independent of the strategy that is used by the system administrator for managing the data whether it is replication or partitioning. Now what this also means is that this is giving an object lesson to the system administrator. In saying that beyond a certain point, you probably want to replicate and partition. Why? Because failures are bound to happen, and a failures are bound to happen if you partition the data, then a portion of the data corpus becomes unavailable. On the other hand, if beyond a certain point Even if you have partitioned data, if you replicate it, then you're making sure that, even if one replica is down, some of the replica, of the same partition is available for serving a particular client request. And replication, beyond a certain point is important because users would normally prefer complete data or a complete harvest. Take for instance, you're accessing your email through the Internet. You would rather put up with a service being unavailable for some amount of time rather than seeing that you only have access to a portion of your main box. And that's the reason why replication beyond a certain point is better for dealing with giant scale services. On the other hand, for searches, it may be okay to have a harvest which is not complete. So in structuring different services as a giant scale service, one has to make a decision of whether partitioning is the right approach or at what point we may want to partition and then replicate as well. As a concrete example, Inktomi which is a CDN server uses partial replication It uses full replication for email because of what I just said, and that is a user would expect complete harvest for a service like email. But on the other hand, if it is a web cache that is being implemented as giant-scale service, it is okay if it is not replicated.
</li> 
</ul>

<h2>11. Graceful Degradation</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/12.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>The DQ principle is also very useful for managing graceful degradation of service. So DQ defines to total system capacity. So if a server is saturated, meaning that we have reached the limit of the server in terms of DQ. That's a constant. DQ's a constant. And so if you reach that limit, then we have a choice of graceful degrading the service from the point of view of the client. </li> 
  <li>One possibility is we keep the harvest the same meaning that every client request that comes in has complete fidelity in terms of the answers returned by the server. So D is fixed. Q comes down because DQ is a constant. That's one option.</li> 
  <li>The other option is to keep the volume of clients that are service, that is the yield Q to be a constant, but decrease the harvest. So the fidelity of the results returned to the users is less than 100%, but we keeping more of the user community happy by serving more of them.</li> 
  <li>Because DQ is a constant, it allows us to gracefully degrade the service being provided by the server depending on the choice you want to make in terms of fidelity of the result or the yield that you want to provide the user community. In other words. The DQ principle gives us an explicit strategy for managing saturation.</li> 
  <li>So as a system administrator, when we make these decisions on which to sacrifice and which to keep constant, we know how our decisions are going to affect the harvest, how it's going to affect the yield, how it is going to affect the up time and so on. So the choices that a system provider has or strategies that a system provider can use in structuring the servers, knowing that DQ is a constant, is when the server is saturated. They can do cost based admission control. You pay more, you get more. That may be one way to do it. Or priority or value based admission control. That may be another way to deal with service saturation or reduce data freshness. That is the harvest may be the same, but you reduce the fidelity of the data. For instance, if I'm serving videos, and I can serve the videos at different bit rates, if the server is saturated, I might decide to serve the video to all the users, all the videos that they want, but at a lower bit rate. So that is reducing the fidelity of the data that is being harvested. So that's the idea behind the DQ principle, how it might be used for graceful degradation of service when the server is saturated.
</li> 
</ul>

<h2>12. Online Evolution and Growth</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/13_1.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>As a giant scale service provider, another thing that the service provider has to worry about is online evolution and growth of the service because services are evolving continuously. The Gmail that you're accessing for getting your email today may not be the same that you accessed a month back. So, since the services are continuously evolving, the servers at the data centers have to be upgraded with a new version of the software or maybe new nodes are being added, or maybe all the old nodes are being retired, and brand-new nodes are being brought in to serve the user community. Whether it is hardware or software upgrade that needs to be done, that has to be done with the understanding that while that upgrade is happening, there is going to be loss of service. Again the DQ principal comes in handy in measuring what exactly is the loss that we're observing of the service when we do this kind of online evolution and growth.</li> 
  <li>The first choice is fast reboot. That is, bring down all the servers at once, upgrade them, and then turn them back on. So this diagram is showing you, time on the x axis, and the loss or the DQ loss on the y axis, and what you're seeing here, this is the time for the upgrade per node. So the amount of time a node is going to be down in order to do that upgrade, whether it is a software upgrade or a hardware upgrade. And this y axis segment is the DQ loss per node. So if I bring down a node and upgrade it, it's going to be down for some amount of time. And this is the amount of server capacity that is lost, the DQ that is lost, as a result of one server being down. If all the servers are down, which is what is happening with fast reboot, then for this entire time, the area bounded by this green-shaded rectangle, we're going to have no service. The ideal is here in terms of the total amount of DQ we want, and the access is saying how much is the DQ that is lost and what we are saying is, if we do this fast reboot, that fast reboot is going to bring all the servers down for a certain amount of time that it takes to upgrade them, and for the entire duration, the service is unavailable. </li> 
  <li>This idea of fast reboot is particularly useful in situations where we can use diurnal server property. For instance, if you think about services that are being all across the globe. We can use factors such as oh, this is nighttime for the user community and the users have probably gone night-night and therefore, they are not going to access the servers. This is a good time to bring down the whole service and upgrade the service. So that might be a situation where fast reboot is very useful. So we are assuming that the user community is segmented so that they're directed to geographically different locations, and we can use the diurnal server property to do this off-peak fast reboot of chosen replicas of the services.</li> 
  <li>In this figure, this segment that I'm showing you here is the DQ loss per node. And the total DQ losses, of course, n times DQ, where n is a number of servers that are undergoing this fast reboot. So in other words, for u units of time, there is complete loss of DQ with fast reboot.</li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/13.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>An alternative to fast reboot is what is called a rolling upgrade. Now here, what we're doing is, rather than bring all the servers down at the same time, we are bringing one server down at a time. And upgrading one server, bringing down the next server, upgrading it, and so on. This is what is called rolling upgrade or wave upgrade. And in this case, services available all throughout, there is no time that we are saying that the service is completely unavailable. But, for the entire duration of time, the duration of time here is n times u, because u is the upgrade time per node, and the upgrade, because it is a wave upgrade, is going to last for n times u time units, where n is a number of servers being upgraded in this fashion. But during the entire time, service is available, but in every u units of time, there's a DQ loss of this much bounded by this area during the entire upgrade process.
</li> 
</ul>

<h2>13. Online Evolution and Growth (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/14.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>A third alternative is what is called a big flip. In the big flip, what we are doing is we are bringing down half the nodes at once. So in other words, the service is available at 50% of the full capacity. So you can see that with a big flip, half the nodes are down. So we have reduced the server capacity, the total DQ available by 50% for u units of time. And the total upgrade is going to last 2 times u because we have partitioned the entire server capacity in two halves, and upgrading one half, upgrading the second half for 2 times u time units we're not going to have full capacity. And during this 2 times U time unit we get the service at 50% capacity, and at the end of this it is back up to full capacity. So these are the three different choices that you've got, the fast reboot where the upgrade time for the entire service is just equal to the upgrade time for a single node. But here, we don't have the service available for any of the users for that period of time, and this might be a good strategy for services that can exploit the diurnal property of user community.</li> 
  <li>Rolling upgrade is the more common one, where what we are doing is we are upgrading the servers one at a time but it's going to take a long time especially when you're talking about data centers having thousands and thousands of processors. Rolling upgrade is going to take a long time and it might be done in batches for instance instead of being exactly one at a time. And the extreme of that rolling upgrade is this big flip where we are saying we'll bring down 50% of the nodes, upgrade them, and then turn them back on, and then do the upgrade for the remaining 50%.So in the third case, in the big flip, the service is always available, but at 50% capacity for a certain duration of time.</li> 
  <li>There is no way to hide the DQ loss. And you'll see that the DQ loss, which is shown by the area of this shaded rectangle in each one of these cases, the area in the shaded rectangle is exactly the same. In other words, the DQ loss is the same for all three strategies, and it is the DQ loss of an individual node, the upgrade time for an individual node, and the number of servers n, that you have in your server farm. That's the total DQ loss for online evolution, and all that these different strategies are saying is, as a system administrator, you have a choice on how you want to dish out the DQ loss. And make it apparent and not apparent to the user community. In this case, it's going to be apparent to the entire user community that there is upgrade going on. In this case, different segments of the user community are going to notice that the service has become unavailable for a short duration of time. And here it's sort of in between these two extremes and that half the user community may see a service being unavailable for some amount of time. So, in other words, using the DQ principle, maintenance and upgrades are controlled failures that can be handled by the administrator of a system service.</li> 
  <li>So what we are seeing through this concept of DQ is a way by which a system developer and a system administrator can work together in figuring out how to architect the system in terms of how the data should be partitioned or replicated and how the system should fine-tune the service by looking at the instantaneous offered load and tuning whether to keep the yield the same or the harvest the same, knowing that DQ is a constant and the server is getting saturated. And finally, when service has to evolve once again, the system administrator can make an informed decision on how to do this online evolution by controlling the DQ loss that is experienced by the user community at any point of time.
</li> 
</ul>

<h2>14. Giant Scale Services Conclusion</h2>
<ul>
  <li>The key insight is that giant scale services are network bound and not disk I/O bound. This is what helps in defining the DQ Principle. The DQ Principle really helps the system designer in optimizing either for yield or harvest for a given system capacity. Also, it helps the system designer in coming up with explicit policies for graceful degradation of services when either servers fail or load saturation happens or upgrades are planned.
</li> 
</ul>

# L09b: MapReduce
<h2>1. MapReduce Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/15.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>We saw the system's issues at a macro level from the point of view of administering the engine that powers internet scale computing. How do we program services such as websearch and airline reservation on top of the data center cluster resources? How do they exploit the hardware parallelism that's available in these clusters? We will learn about the map produced programming paradigm that answers these questions.</li> 
  <li>The term Big Data has become a buzzword. Computations in giant scale services are usually simple, but they work over large datasets and hence the name Big Data. And naturally, because they are working with a large data set, these computations take a long time to compute. For example let's say that I want to search for John F Kennedy's photographs in all the documents that are available on the web. It's going to take a long time. And that is an example of a Big Data computation. Similar examples include online reservations, shopping, et cetera. And applications that work on Big Data would like to exploit the parallelism that's available in the cluster at the data centers. I mentioned in the last lesson that data centers these days comprise of computation elements on the order of thousands and even 10,000 nodes that are ready to do computations. And we would like to exploit the computational resources available in such Big Data centers to do computation on Big Data. For example, if it is John F Kennedy's photographs, I'm looking in all the different documents. We can parallelize that's search for Kennedy's photo in all the documents in parallel on all the available nodes in the data center. And since computations are also called embarrassingly parallel computations. What does this mean? Well, there is not much synchronization or coordination that is needed among the parallel threads that comprise the applications and that run on different nodes of the computational cluster. Looking for John F Kennedy's photographs in all the documents on the web is a good example of such an embarrassingly parallel application. What are all the issues in programming in the large on large data sets, so the data sets are also large and the computational resources on which we want to run this big data application is also vast.</li> 
  <li>So some of the interesting issues in programming in the large include how to parallelize an application across let's say, 10,000 machines. How to handle the data distribution and plumbing between the producers of data and consumers of data. Remember these are embarrassingly parallel applications, but different phases of the application will require data from one phase of the application to be passed on to the next phase of the application. That's what we mean by plumbing between the producers of data, intermediate data to be more specific, and consumers of that intermediate data. And of course, one of the biggest challenges in Big Data applications on large computational clusters is failure handling. Recall what we said about the nature of these data centers. They employ thousands and thousands of processes. When you have so many parts, in any setting, failure is not a question of if it will happen. It is a question of, when it is going to happen? So that's a fact of life, and therefore, programming models for Big Data have to worry about the fact that failures are to be expected in this environment.
</li> 
</ul>

<h2>2. MapReduce</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/16.JPG?raw=true" alt="drawing" width="800"/>
</p>

<ul>
  <li>In this lesson we're going to look at one specific example of a programming environment for dealing with big data applications running on large computation clusters. And this programming environment is called MapReduce programming environment. And in this programming environment, the input to the application is considered as a set of records Identified by a key value pair. So the big data app developer, supplies the run time environment with two functions called map and reduce. And these are the only two functions that the app developer has to supply to the programming environment. Both map and reduce take user defined key value pairs as inputs and produce user defined key value pairs as outputs. So both the input and the output to each of the map and reduce functions, written by the domain expert, are key value pairs. Once again, these are key value pairs defined by the app developer, specific to the particular application that he or she is coding.</li> 
  <li>Let's consider a fun example. In this example, I'm going to say that we're looking to find specific names of individuals in a corpus of documents. Maybe the names are Kishore, Arun, Drew, and so on. These are the specific names that I want to find in a whole corpus of documents. So the input to the application is a whole corpus of documents, and we are looking for specific names and the number of times those names occur in those documents. So that's what the appllication is going to do. So the input key space for the application is the file name. And the value part of the key value bares the content of the file. So if you have n files in the corpus of documents that we want to analyze in this fashion, then you have N key value pairs corresponding to each of the different files and the respective contents. So this is going to be the input to the whole application and were going to structure this application using this programing paradigm of map reduce. And we will see how the app developer will use the map-reduce framework to accomplish the goal that we set out. Which is to find the number of occurrences of unique names, in this corpus of documents. In this example, the user defined map function is looking for each of the unique names that we're interested in the corpus of documents. So the map function will take as an input a specific file and the contents of the file as the key value pair, and emit a value that corresponds to the number of times each one of these names occur in the input file. Now, a simple version of the map function may emit a one every time it encounters the name Kishore in the input file, or the name Arun in the input, or the name Drew in the input file. The output of the map function is a key value pair, where the key is the unique name that you're looking for. One of the unique names that you're looking for, and the value is the number that says how many times it found that particular name. As I said earlier, a simple version of the map function could emit the value of 1 every time it finds one of the specified names that it is looking for in the file. Or a slightly more elaborate mapper may actually combine the number of occurrences of a particular name in the input file and then emit that as a sum of all the times a particular name occurred in this key value pair. In either case, the output of the map function is itself a key value pair. Note that it is different from the input key value pair. The output key value pair that the mapper is emitting is a unique name and a value that is associated with that unique name and from this example it should also be evident that we can spawn as many instances of the map function as the number of unique files that we have in the input document corpus. Further, since each of the map function is working on a different key-value pair as an input, they're all independent of one another and there is no need for coordination among them. And that's the feature of an embarrassingly parallel application. The output of the mappers are the input to the reducers. In other words, the output key value pair from the mapper is the same as the input key value pair for each of the reducers that you see here. Again this reduce function is something that is written by the user. And you notice that what the mapper is doing is when it notates a particular name in this example that it is looking for like Kishore then this mapper is going to send the value associated with Kishore to this reducer. Similarly this mapper is going to send the value associated corresponding to Kishore to the same reducer. And all the mappers in the same fashion are going to send the respective values that they phone for this unique name Kishore, to this reducer. Similarly, the values phone for the name Arun, is going to be sent to this video set by all the mappers, and so on. So the number of reducers that we will have is the same as the number of unique names that we are looking for in the corpus of documents. This is where work needs to be done by the programming environment to plumb the output of the mappers To the inputs of the reducers. Potentially, the mappers could be executing on different nodes of the cluster, reducers can be executing on different nodes of the cluster, and the work that is involved in this plumbing is really making sure that the outputs of the mappers are fed to the corresponding correct reducers in the entire distributed system. For example, the output that is coming out of the mapper corresponding to Kishore has to be sent to this reducer from all the mappers. The output corresponding to Arun coming from all these mappers has to be sent to this reducer, and so on. That's what we mean by the plumbing that needs to be done by the programming environment to facilitate the communication That needs to happen between instances of the mapper and the instances of the reducers. The work performed by each one of these reducers is going to be aggregation of the values that it got for each of the unique names that we're looking for in the input document corpus. So this reducer's job is to aggregate all the instances of Kishore that it got from all the mappers. So it is receiving the key-value pair corresponding to the name Kishore and the number of instances that they found in the input key-value pair that they processed, and the reducer is going to aggregate that And the output of the reducers, all the reducers, is once again going to be a key value pair. It is exactly the same as the input key value pair that the reducers got. Namely, the key is the unique name that they've been assigned to aggregate, and the value is the total number of the occurrence of this unique name. In the corpus of documents we started with. So this is the roadmap for creating a map-reduce application for a chosen application that works on big data. So in this case, the chosen application was looking for unique names in a corpus of documents, and all that the domain expert had to worry about, is deciding what is the format of the key value pair, which is the input to the whole application, in particular, the map phase of the application. And also, what is the format of the intermediate key value pair that is going to be generated by the mapper function. And what is the work that needs to be done in each of the mapper and the reducer to carry out the work that needs to be done for this particular application. Beyond that, the app developer does not have to worry about any of the other details, such as how many instances of mappers do we need to create. How do we create the number of instances of reducers corresponding to what this application is wanting to do, and also worrying about the plumbing from the output of the mappers to the input of the reducers, all of those, are things that the app developer does not have to worry about.
</li> 
</ul>

<h2>3. Why MapReduce</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/17.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>The question is why MapReduce is a programming framework for big data applications. It turns out that several processing steps in giant-scale services are expressible as MapReduce. For example, let's say that you're looking for seat availability for selected dates to selected destinations on different airlines. That can be expressed as a MapReduce computation. Let's say you want to access frequency of the URLs on the website that you've designed, that can be quoted up as a map reduce application. Let's say you want to create word indexes to facilitate document searches on the web, that can be coded up as a MapReduce application. Or let's say that you want to do ranking of pages, when a user is doing a search, how to present present the search results may depend on the popularity of pages and that is what is often referred to as page ranking. So if page ranking has to be done for all the documents, that is another application that can be coded up as a MapReduce application. The list goes on.</li> 
  <li>All such examples that are mentioned share some common property. They're embarrassingly parallel, and they're common in giant-scale services. And all of them tend to work on big datasets. Therefore there is plenty of opportunity for taking advantage of the computation resources that are available in a data center.</li> 
  <li>All such applications need domain expertise in terms of what to do with the data, which is expressible as a map function and a reduce function, and this is the only thing that is required of the domain expert to provide to the programming system, because that is domain expertise that lives with the app developer.</li> 
  <li>So here is another example of how an application may be structured as a map_reduce application. And in this case, I'm showing you a page ranking application that is, I'm interested in knowing what is the popularity of different URLs that occur in a document corpus. So the keyspace that is input to this application is a set of URLs. And the key value pair that is coming into each of the mapper is a source URL and the contents of that web page that corresponds to this particular URL. So what this mapper is doing is, in the given page defined by this URL the contents of which is the input to this mapper it is looking for different targets. Maybe it is looking for a particular URL target one, another URL target two, and so on, target N, and that's what each of these mappers are doing. So the keyspace that is output from the mapper is unique target names. So the keyspace that is output from the mapper is unique, target URLs and the value is the URL in which it was actually phoned. So the corpus of input URLs it is taking, and saying well, in this particular URL I phoned this target. If it did. It is emitting that this target was found in a particular URL. And this reducer is going to get all the hits for a particular target that was found in the input corpus of URLs. So all the mappers they're going to send their results to this reducer if they found in the input - Surl the target, target 1, if they did, they are going to send their results to this reducer. Similarly if they found target n, each of these mappers are going to send this reducer that in the input URL they found this target n. And the job of the reducer is once again aggregation, and you have as many reducers as the number of unique targets you're trying to identify in the input dataset. Very similar to the previous application that we went over. So the output of the reducer is going to be the specific target that this guy has been asked to accumulate. And, a source list, meaning all the source pages in which this particular target was found. So each of these reduces is going to find the number of times a particular URL is found in the input corpus of webpages that came into the system as a whole. For instance, if I wanted to find out how many times my webpage appears in the universal web pages all over the world, we can take the entire corpus of web page available in the universals input, and the map that's we're going to look for occurence of my web page in each one of those input web pages. And if they find that, they're going to send it to this reducer and if target one corresponds to my web page then this reducer is going to say, "okay, show his webpage, was found in this list of source webpages all over the Universe." That in a sense gives a rank for. That particular web page that we're looking at. So we're able to rank the target web pages 1 through n based on the number of source web pages that contain that particular target. And that's what page ranking is all about. So I'm giving you yet another example of how this map reduce functionality can be applied for an application such as page ranking.</li> 
  <li>All the heavy lifting that needs to be done, in terms of instantiating the number of mappers, instantiating the number of reducers, The data movement between the mappers and the reducers, all of those chores, are taken care of by the programming environment. All that the domain expert had to do was to write the map and reduce function that is unique to particular specifics of his application. The rest of the heavy lifting is all done by the programming framework.
</li> 
</ul>

<h2>4. Heavy Lifting Done by the Runtime</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/18.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Now let's look at all the heavy lifting that needs to be done by runtime in order to facilitate this MapReduce style of programming. The app developer writes his map function and reduce function, and instantiates the programming library by calling MapReduce. And the programming library splits the input files that is provided by the user, that is a key value space provided by the user into M splits. The number of splits of the input key value pairs,namely M, can be automatic by the programming system, or M can also be specified by the user. In any case, once M is specified or automatically determined by the programming framework, it splits the input key value space into M splits.</li> 
  <li>The next thing that happens in the programming environment is to spawn the master for this particular MapReduce computation, and all the worker threads That are going to carry out the work involved in map functions and reduce functions. The master is special because the master is the one that is sort of overseeing the whole operation and keeping tab on which workers are doing what functions, when are they done,when to start new work, when to say that the computation is done. All of those chores are being orchestrated by this master.</li> 
  <li>The next thing that happens is that the master is going to assign some number of worker threads as the mapper worker threads and the number of mapper working threads may correspond to the number of splits that it has done in the beginning. So there are M splits, then M worker threads are going to be assigned to mapping function. So each worker thread is going to take one portion of the input file split that have been done, and apply the map function on that particular input split. The next thing that happens is that the master assigns reduce tasks to some number of worker threads and the number of reducers to be assigned to the workers, R, is something that is decided by the application. Recall in the example that I gave you about looking for specific names. In an input corpus, the number of unique names is something that the app developer is specifying. That's where the number R comes from. That's the number of splits that the user is specifying, and that parameter is going to be used by the master to assign some number of workers as reducers.</li> 
  <li>The next thing that the master does is to plumb the mapper to the reducers. Because now when the mappers produce that output. That output has to be sent over to the consumers of the intermediate results of the mapper, namely the reducers. And setting up this communication path between the producers of data The mappers and the consumers of data that it uses is the plumbing that the master does as the next thing.</li> 
  <li>Now it's time to get to work. The map phase, what it is going to do is, it is going to read its respective split. So each of these workers is assigned to the mapping function. So each of the worker is working on a particular split. And what they're going to do is read from the local disk, the split that they've been assigned, parse the input, and then call the user defined map function. The user defined map function is the one that is doing the core of the work that needs to be done on the input data set to produce the intermediate output. The rest of it are things that needs to be done in order to facilitate the work to be carried out by the domain expert in the map function. And the intermediate key value pairs that we produced by the mapper will be buffered in memory, so each one of these workers is doing a portion of processing the input key value place and producing the respective outputs. And periodically the intermediate results are going to be written to files, which are on the local disks of the worker or the respective workers. For this guy, on its local disk, its going to write intermediate files corresponding to the output of the map function. Similarly this worker is going to write to its local disk, the intermediate files and so on. And because the application developer has specified that there are R splits in the reducers. Each worker, meaning each map function that is associated with that worker, is going to produce R intermediate file. One for each of the R workers that are going to be doing the reduce operation. And all these intermediate files are stored on the local disk associated with each of these computational nodes carrying out the worker function corresponding to the map phase. And when they are done with the map operation for the split that they are handling, the worker will inform the master "I'm done." And what the master's waiting on is waiting on all of these mappers to get done before letting the workers to get going on the input data set.</li> 
  <li>So in this sense, the master is like the conductor of an orchestra. He is started this mapping function waiting for all of these guys to get done, and when they indicate that they have done the work by notifying the master. And all of the M mappers that have been assinged to these workers have completed that work, then the masters say, "okay now it is time to move on to the reduced phase."
</li> 
</ul>

<h2>5. Heavy Lifting Done by the Runtime (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/19.JPG?raw=true" alt="drawing" width="700"/>
</p>


<ul>
  <li>In the reduce phase, there is more work to be done in pulling the data that is needed for each one of these reducers. Because we know that the mappers have been executing on different nodes of the computational cluster, and they produce their intermediate results as files on their local disk. And this worker that is carrying out a particular split of the reduce operation has to reach out and pull the data from all of the m mappers that have stored their intermediate results on their respective local disks. So there is remote read that is involved.</li> 
  <li>As the first thing in the reduce phase is to pull the data. This is part of what I mean by the plumbing that the runtime system provides is to recognize that, for this reduce operation to work, it needs the mapping results from all the m nodes that carried out the map function. And so it is going to do RPC in order to get all this data from all the local disks of the nodes on which the map was executed. And once it has all the data, it can sort it, and then call the reduce function. The reduce function is the one that has been written by the domain expert. And this is the point at which the domain expertise comes in, saying, "well, I've got the data now, let me do the processing that I want to do for the reduce operation." The sorting that is being done as part of the programming framework may be to sort the input that is coming in from all of these different mappers, so that all the same keys are together in the input data set that is going to be given to the reduce function.</li> 
  <li>Once such sorting has been done, the programming framework will call the user-supplied reduce function for each key with the set of intermediate values so that the reduce function can do its thing, which is domain specific. Each reduce function will then write to the final output file specific to the partition that it is dealing with. So, if you think about the original example that we started with, if this guy is accumulating all the instances of the name Kishore, then it will write the output file that says "oh, I've found so many instances of the name Kishore in the input corpus of data." And similarly, this guy may be doing it for another name like Drew, or Arun, and so on. And once each worker has completed its work by writing its final output file for this partition that it is responsible for, then it informs the master, "yes, I'm done with the work that was assigned to me." The user program can be woken up when all the reducers have indicated to the master that they have done the work, and at that point the map reduce computation that was initiated by the user program is complete.</li> 
  <li>We said that there could be m splits of the input dataset, meaning the input key-value pairs, and there could be R splits of the output that has to be generated by the application as a whole. Now, the computational resources that are available in the data center, N, may be less than the sum m plus R. So there may not be a unique machine to assign for each one of the m splits that have been made. It is the responsibility of the master to manage the machines and assign the machines that are available to handing the m input data sets as well as the R reduce splits that need to be generated. So this is part of the heavy lifting that has to be done by the runtime.</li> 
  <li>So, for instance, let's say that I have only 100 worker nodes available as mappers. And I have an input split of 1000, then I'm going to assign one split to this worker. And when the guy says "I'm done with it." then I'd say, "oh, you're done? Okay, take the next split and work on it." So that's how we're going to manage the available resources for carrying out the work that needs to be done. So remember that this is all done as part of the heavy lifting by the runtime, the user doesn't have to worry about it. All that the user did was write the map function and write the reduce function, and the rest of it is magic so far as the map reduce framework is concerned. And similarly, the number of R splits specified by the user may be more than the number of workers that are available to carry that out. And in that case, again, the master is going to assign a particular split to this worker so that he can compute and generate the output file corresponding to that split. Once he's done with that and notifies the master, then the master will say, "okay, now that you're done with that, work on the next split." And it'll do all the work that is associated with plumbing, meaning, bringing the data for the split that a particular worker is working on right now, sorting it to put all the corresponding keys together, and then finally calling the reduce function that has been written by the user. That's the kind of work that goes on under the covers in the programming framework of MapReduce.
</li> 
</ul>

<h2>6. Issues to be handled by the Runtime</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/20.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Lots of work to be done by the runtime system to manage and map_reduce computation. The master data structures include the location of files created by the completed mappers. Remember that each mapper is working on some node of the computational cluster producing its output as files on its local disk. So the master has to have knowledge of where those files reside created by those mappers, and the namespace of the files that have been created by the mappers.</li> 
  <li>And it also has to keep a score board, of the mappers and reducers that are currently assigned to work on different splits. Because the number of machines that may be available, may be less than the total number of machines that will be needed. If I wanted to do all of the input m splits in parallel and all of the R output splits in parallel. And therefore, the scoreboard is saying, "at any point in time, who are all the workers carrying all the mapper functions? Who are all the workers carrying all the reducer functions? When are they done? And when they are done, how should I reassign that worker to a new task?" So these are the kind of things that the master data structures facilitate the master to do.</li> 
  <li>Then the big thing is fault tolerance. For a variety of reasons, the master may find, that an instance, of a map function that it started, is not responding in a timely manner. Maybe that node is down for some reason, maybe the link is down for some reason, or maybe that processor is taking a little more time than what the master expects. It can happen very easily, because in data center environment where you have thousands of machines, there could be some machines that are slower than other machines, there could be generational differences between the machines that populate the data center, even though they may be homogenous in terms of the capabilities. Programming alignments and even the machine architecture. They may have speeds that are different because they correspond to different generations of processes. In any event, what might happen is that a master may notice that there is no timely response from a particular instance. Let's say of a mapper. In that case, it might say well I'm going to go ahead assume that that mapper is dead. I'm going to restart that mapping function on a different node of the cluster. Now when that happens, it is possible that the original mapper is actually dead, or it could be that it was just slow. In that case, you could get completion message from more than one mapper for the same split. So when you get multiple completion messages from redundant stragglers, the master has to be able to filter the them out. "This is something that is redundant work. I don't have to care about that." So that's part of fault tolerance that the master has to worry about.</li> 
  <li>Locality management is another important thing. In the memory hierarchy, making sure that the working set of computations fit in the closest level of the memory hierarchy of a process is very important, and this is another thing that has to be managed very carefully so that the computation can make good forward progress in completing the MapReduce function. There is an inherent assumption in the fault tolerance model of the map produce framework. And that is item potency of the mapper. That is, even though the same input data split is being worked on by multiple mappers, It doesn't affect the semantics of the computation as a whole, and that is the reason why the master can simply ignore the redundant stragglers' message if in fact it was a slow computation engine that was working on a particular map function. And if it finishes later than when it was supposed to, and the master has already started another redundant computation to take care of that particular split, ignoring that redundant straggler message is okay because of the item potency assumption about the mapping operation. In a similar manner, the master may assign a new node to carry out reduced function corresponding to a particular split if there's no timely response from one of the reduced workers. And here again, the output file generation that is associated with a particular. A reducer split depends on the atomicity of the rename system call that is used to finally say that "oh, this is the old profile generated", and the master says "okay, I'm going to commit this as the real output file of the computation." Each reducer is writing a local file, and finally when the master gets the notification from the reducer, that it has completed its work, at that point, master renames that local file as the final output file, and this is where can get rid of redundant stranglers who come along later and say "I produce the same output file for the same split." Master will say, "I already got it, I don't need this." and it can ignore the completion message from that redundant straggler.</li> 
  <li>The other thing that the programming environment has to worry about is locality management, and this is accomplished using the Google file system that provides a way by which, efficiently, data can be migrated from the mappers to the reducers.</li> 
  <li>Task granularity is another important issue. As I mentioned, the number of nodes available in the computation cluster may be less than the sum M plus R, where M is the number of input splits, and R is the number of Reducer splits. And it is the responsibility of the programming framework to come up with the right task granularity so that there can be good load balance of the computational resources. And the programming framework also offers several refinements to the basic model. The way the partitioning of the input data space is done, is by using a hash function that is built into the programming environment. But the user can override that partitioning function with his or her own partitioning function if they think that that'll result in a better way of organizing the data. And in the map reduced framework, there is no interaction between the mappers, but if there's some ordering guarantees needed in terms of ordering the keys and so on, that is something that the user may have to take care of. The other thing that the user may also do is combining. A partial merging to increase the granularity of the tasks that execute a mapping function. Remember that when we looked at the simple example of looking for specific names and input corpus of data, I mentioned that the mapper can be as simple as emitting a one for a value every time it encounters the name Kishore for instance in an input file. Or, it could take an input file and count all the occurrences of Kishore in that input file and finally emit the total value as the output of the mapper that kind of combining function is something that a user may choose to incorporate in writing his mapping and reduce functions. It is very important to recognize that in order for the fault tolerance model of the programming environment to work correctly, map and reduce functions have to be item potent. That's a fundamental assumption. That the fault tolerant monitoring of the programming framework is based on. And there are also other bells and whistles in the programming framework for getting status information and logs and so on and so forth, but the basic idea I want to leave you with is the fact that the programming framework is a very simple one. It has two operations, map and reduce and using those two operations. You can specify a complex application that you want to call up, to run on a data center, using the computation resources that's available in the data center.
</li> 
</ul>

<h2>7. MapReduce Conclusion</h2>
<ul>
  <li>The power of Map Produce is it's simplicity. The domain expert has to write just the Map and Reduce functions for his application. All the heavy lifting is completely managed by the runtime system under the covers.
</li> 
</ul>

# L09c: Content Delivery Networks
<h2>1. Content Delivery Networks Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/21.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>The Internet and the World Wide Web and intimately tied to everyday lives. It has put information that we seek at our finger tips. Remedies for the common cold, dinner recipes, sports updates, developments in the Middle East, you name it. You will find it on the internet. We know that the creation of such content happens from the basement of individuals like you and me, as well as businesses such as CNN, BBC, and NBC who's work it is to create such content. How is the content in the internet stored and distributed?</li> 
  <li>In the first two lessons of this module, we looked at the server end of the giant scale services. How's a computational resources organized in the data center? How is the data organized internally in the computational nodes of the cluster? And what is the programming model for dealing with the big data associated with giant scale services and for exploiting all the computational resources that are available in the server forms and data centers? In this lesson, we'll look at content distribution networks. That is How is information organized, located, and distributed when we're looking for them? Once again, the issue is dealing with the scale of the problem. The content is generated worldwide and users are trying to access the content worldwide as well.
</li> 
</ul>

<h2>2. CDN's</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/22.JPG?raw=true" alt="drawing" width="700"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/23.JPG?raw=true" alt="drawing" width="700"/>
</p>


<h2>3. DHT</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/24.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Lets understand what content distribution networks are. I'm planning a holiday trip to India fairly soon and let's say on the holiday trip I record some nice videos of some of the places that I went sightseeing. And let's say I call the content that I generated thusly.Kishore's India trip. To keep it simple, let me call the node ID of my computer as 80. And I want to publish this video that I generated of my interesting visits to some of the sights and sounds of India on the Internet, so that anyone on the Internet can find it and download it.</li> 
  <li>How do I name the content of my video? Well, textual names may not be very meaningful, because there could be name collisions. So what I'm going to do is, I'm going to create a content hash of the video, and let's say my content hash has enough bits to ensure uniqueness of the hash string that I generated, I'll call that the key. Now I have key value pair the key is a bit string that is a hash of the content of what I want to store and make available to everybody, and the value is my node ID where the content is stored. Lets say that the content turned out to be 149, and my node ID is 80. So the key value pair that I've generated for my interesting video is <149, 80>. 149 is the key, and 80 is the value, which essentially is the node ID of my computer where I have the content. In other words the key 149, Is a unique name that I've generated for the content which I want to distribute to others that may be interested in looking at this particular content. Now if you get this key value pair <149, 80>, you know that 149 is the key that uniquely names my video it shows India trip and 80 is the value which says where you can get this particular video from. So you can then come to my node and get it from me.</li> 
  <li>Now the question is where to store this key value pair, so that anybody can discover this key value pair. If they are looking for Kishore's India trip, maybe I published the fact that Kishore's India trip unique name is 149 somewhere. And so now, if, anybody wants to find a way of locating the server from which they can download the content they need to get this key value pair. So, when I created this particular key value pair, I have to find a way to place it in the internet. So that anybody can get these key value pair, and from that know the node from which they can download the content. Now we cannot put this on a central name server because it does not scale. Because user generated content is proliferating on the internet. So using a central server to store. All the key-value pairs is just not scalable. We need a distributed solution.</li> 
  <li>This is where the idea of DHT, or distributed hash table, comes into play. The idea is quite simple. Anyone who generates a key value pair has to find a location where they can store it so that intuitively, anybody that is looking for that particular key will be able to discover it. So if I want to store a key value there, what I'm going to do is find a node whose ID is exactly the same as the key itself. Or if not, close enough to the key that I want to store. So in this case, the key that I generated, which is a content hash of my India video, is 149. And looking on the internet, I find that there is a node whose node ID is 150. Close enough to 149, and therefore, I'm going to store this key value pair <149,80> in this particular node 150. Now, if you're looking for my India trip video, you know that the unique signature associated with that is 149, and you will know because of the structure of the DHT that the place to look for the signature 149, or the key 149, Is a node whose address is also equal to this key value or close enough. 150 is the one that is close enough so you'll go to 150 and from 150 you will get this key value pair, and once you get this key value pair, you will know that all the content is stored in node 80 and then you come to me and get the content from me. This is how content distribution networks exploit the distributed hash table technology to store content on the internet, so that they can be discovered and disseminated to the users.
</li> 
</ul>

<h2>4. DHT Details</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/25.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Let's dig into some of the details of the DHT technology.</li> 
  <li>The first namespace that the DHT technology has to deal with is the key-space namespace. So the way namespace is managed is, if you have content to disseminate, you generate a unique key for the content by using some algorithm like SHA-1 which generates a 160-bit key. The number of bits in the key or the signature is big enough that they will not recollision even if different content are using the same algorithm to generate this key. So that is how you create a unique signature for a particular content, using SHA-1.</li> 
  <li>The second namespace we have to deal with is a node-space. So here, what we're doing is, we are creating an SHA-1 hash of IP addresses of nodes that want to share content. So, let's say that me and my buddies form a social network and all our IP addresses, we going to use this algorithm to encode them into this 160-bit node id. So now we've got two namespaces, one is the key-space namespace and the node-space namespace, and both of them are derived using the same algorithmic technique, let's say. So in this case we've created a key for the content so that there's a unique signature for a particular content, similarly, we've created a node id from the IP address, which is again a unique signature for a particular IP address.</li> 
  <li>The objective is, if I have a key, I want to store that key in a node id N, such that the key is very close to the node ID N. Ideally, if you're lucky, the key is exactly equal to N, but, you know, it's not possible to guarantee that this hash and this hash will result in exactly the same value. So long as it is close enough, like in the previous example I showed you. That if I generated a hash 149, I stored it in that node 150, which is close enough to the hash that I created.</li> 
  <li>So the API for manipulating this distibuted hash table data structure would be putkey and getkey, so putkey would take two arguments: the key and the value. Value can be anything that you want to associate with that. In the previous example, I've said the value may be the IP address of the content that is associated with the particular key. And getkey takes one argument, namely the key, and returns the value that is associated with that key-value pair.
</li> 
</ul>

<h2>5. CDN (An Overlay Network)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/26.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>CDN is an example of what is called an overlay network. Let me explain what an overlay network is. In the previous example, I said my node id is 80. And the content corresponding to the key 149 is stored at my node id 80. And this is something the chip discovered, and once you've discovered it, you want to come to me to get the content from me. But how will you do that? What does 80 mean from the point of view of the internet? As you know, the physical infrastructure of the internet works with IP addresses. But what you have is 80. It's not an IP address. And the operating system only understands IP addresses to route packets on the Internet from source to destination. So we need, at the user level, because we are doing this sharing of content at the user level, a way of mapping such virtual addresses 80, 60, and so on, to IP addresses. So we, as friends, who have decided "well, we want to form a social network to share information content." We've exchanged such mapping information and constructed a routing table at the user level. And this is what is called an overlay network, a virtual network on top of the physical network.</li> 
  <li>For example, if you, using that SHA-1 hash, found that your IP address maps to node ID 60, you'll say, "okay, my node ID is 60. And my IP address is such" And so that is how I'm going to construct a user-level routing table. All I know is your node ID, and that node ID maps to some IP address. And I can use this correspondence between the node ID and the IP address, to have a way of sending information to you. And maybe, a friend of yours has exchanged routing information with you, and he has told you that his node ID's 80. And he has also given you how to reach him. Given that node ID. And you've shared that information with me as well saying that you know, I have buddies, and these are all the note IDs of my buddies. So that's how I construct this routing table, which is really a table that consists node IDs of my friends and friends of friends, friends of friends of friends, and so on. That's how I construct this user level routing table.</li> 
  <li>If I wanted to send a message to my buddy, let's say B, and his node ID is 60. Because B has given me his IP address, when I send a message to node ID, I can covert that to the IP address, give it to the physical network, and it goes to the physical network gets delivered to B. What if I want to send a message to some other node C. I know the node ID of c because my buddy b exchanged that with me. But I have no idea what the IP address of C is. But on the other hand, I do know that B has a way of getting to C. And so in my routing table, what I'm going to say is that given a name of a particular node, I know the virtualized node ID associated with that node, 64B, 84C, and I also know what the next hop is in the user space, in the virtual space of getting to that particular destination. In the case of B. I know I can directly send it to 60, because I have an IP address for B. On the other hand, if I want to send it to C, I have the node ID of C, but I have no idea what that maps to in terms of IP address. I know that my buddy B has a way of getting it to C, and that's what I'm going to use. I'm going to say, if I want to send something to C, I simply hand it over to my buddy who is at node id 60. So the routing table says, given a node id, who's the next hop I should give it to so that it'll eventually get there? </li> 
  <li>The physical network is much more elaborate than this user level, overlay network. Now, for instance, let's say I want to send a message to C. His node ID is 80, but I have no idea how to send it to him except to know that if I give it to my buddy B, who's at node ID 60, he'll know how to get it over to, to C. So, I'm going to send it to B, and when it comes to B, B knows. From his routing table, the IP address for C, and therefore he will send it to C and it'll eventually reach C. So, at the user level, you can see that so far as I'm concerned, if my node ID is 50, and my node name is A, when I wanted to send a message to C, what I did, was to send it to B. And it took two hops at the user level. It took two hops to go from A to C, went to B my buddy, and B then sent it over to C. So this is a user level traversal of the message. But in reality, under the covers, what is going on, is when I send this message from A to B, it is going through the physical network and reaching B. And when I send to message to A to C. Even though it is two hops over here, internally in terms of the number of network hops that may have to be incurred in the physical network for the message to reach eventually the destination C, it might take many more hops.</li> 
  <li>That's what an overlay network is. A virtual network on top of the physical network. And this particular overlay is a content distribution network, because it allows content to be shared and distributed among a set of users, who have exchanged information with one another so that they can discover one another, if not directly, indirectly through friends of friends.
</li> 
</ul>

<h2>6. Overlay Networks in General</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/27.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Overlay Networks is a general principle and at the OS level we already have an overlay network. There is the IP Network which is really an overlay on top of the local area network. You may have a particular IP address for your laptop. That IP address actually translates to a Mac address. Because the Mac address is the real address that is used by the physical network in the local area network to communicate with other buddies on the same local area network. So at the OS level, when you send a message using TCP/IP, that message will be delivered to your buddy who happens to be on the same local area network. It is actually getting converted to a Mac address so that it can traverse the local area network and get to the desired destination. So that it can traverse the local area network and get to the desired destination.</li> 
  <li>Similarily, CDN, a content distribution network is an overlay on top of TCP/IP. So in particular, In the case of CDN, we have this node ID. There is some manufactured ID at the application level, but that maps to an IP address, and that is how at the application level, when I say I want to send a message to Node number 60, at the application level I can convert that Node ID 60 to an IP address, to which I can send it and eventually the message will get delivered.
</li> 
</ul>

<h2>7. DHT and CDN's</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/28.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>DHT or distributed hash table is an implementation vehicle for a content distribution network to populate the routing table at the user level. As I mentioned earlier, for placement you need a put operation. And the put operation is going to take 2 parameters, a key and a value, and the key is some content hash that uniquely identifies something that the user community may be interested in finding. And value is the node ID where the content is stored. </li> 
  <li>The retrieval of a key value is done using a get operation. And when you do a get and give the key, what you're expecting to get back is the value that is associated with this key value pair. So, what you get back is a value that is associated with this key, which was placed somewhere using the put operation.</li> 
  <li>So using the put and get operation, let's see how to construct the routing table as new content gets generated. That is, you want to do placement of key value pairs using put operations, and retrieval of a value associated with a key with this construction.
</li> 
</ul>

<h2>8. Traditional Approach</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/29.JPG?raw=true" alt="drawing" width="700"/>
</p>


<ul>
  <li>The traditional approach which I'll call the greedy approach in constructing distributed hash table is, when you want to place a key value, you pick a node N, where N is very close to key. And now, if you want to retrieve a given key, K, the algorithm you know is going to be, you want to go to a node N, which is closest to this key K, because that is the algorithm that is being used for placing, so for retrieval you just do the reverse. When you want a key K, you go to a node which is closest to key K. That's how these routing tables get populated at different nodes in the distributed system at the user level. So the routing table at A says that these are the known peers to me whose node IDs I know and I know their mapping of the node ID to the IP addresses. Now the node space may be much bigger than the number entries I have in my routing table. So what do I do if I want to communicate with a node whose node ID I know, but I don't have a mapping for that node ID with respect to the IP address. Basically the routing table at every node is just saying these are the nodes that I know how to communicate with directly. That is all the nodes that are reachable from node A. Because, at the user level, I have a mapping between the virtual node ID that is used in the DHT and the IP address that corresponds to it. And remember that IP address is the only thing that the operating system is going to understand. And therefore I know how to communicate with node 60, because I can give the IP address that corresponds to node 60 when I want to send a message to node 60. Similarly for 79. Over here, node B knows how to communicate with 60, knows how to communicate with 109. And if these are the only entries that are in the routing tables of A and B, these are the ones to which node A knows how to communicate directly.</li> 
  <li>What if I want to go to some other node that is not in my routing my table yet? For example, let's say that I am trying to retrieve a key, 58 or 59. I know that 58 or 59, in terms of the DHT construction, it's most likely stored in some node whose ID is very close to this key. Now in my table I have a node ID 60, close enough to the key that I'm looking for. So what I'm going to hope is that if I am looking for this particular key, 58 or 59, good chance that the key value pair that corresponds to 58 or 59 is stored in this node ID 60. So that's the one that I'm going to communicate with. It's possible that 58 is actually stored in a node ID 58, in which case my hope is the desired destination that I want to reach to is known to this peer, 60, who is close to the node number 58. So in other words, if I want to communicate with node number 58, my best bet is to communicate with node number 60 with the hope that 60 may actually know how to communicate with node number 58, because ultimately I'm hoping that that's where this particular key value pair may be actually stored. On the other hand, if the key that I am looking for is 80 or 81, then I'll say well, chances are this key is toward a node ID 79 for whom I have a mapping. Or if I go to him, he might know how to get to node number 80 which may be actually storing this key 80, which may be actually storing this key 80. So in other words, in the greedy approach what we're going to do is in placing a key value we're going to place the key value pair at a node N where N is equal to K, ideally, or close to K. Similarly, if I want to retrieve a key K, I'm going to go to a node N, where N is either equal to K if I know how to get to it from my routing table. Or get to a node that is close enough to the desired N. In this case, the desired N is 58 or 59, but the one that I can get to is 60, and my hope is that when I get to 60 he will know how to get me to node number 58 or 59. Or even better, he may be the one that is storing this particular key value pair that corresponds to the key that I'm looking for. So in other words, in the greedy approach we are trying to get to our desired destination as quickly as possible with the minimum number of hops to get to the desired destination. And when I say the number of hops, it is at the level of the virtual overlay network, not in terms of the physical network. Because at the user level have no idea how many hops my message may actually take going from source to destination. All that we are saying is at the user level we are trying to minimize the number of hops to get to the desired destination.
</li> 
</ul>

<h2>9. Greedy Approach Leads to Metadata Server Overload</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/30.JPG?raw=true" alt="drawing" width="700"/>
</p>


<ul>
  <li>The greedy approach leads to what is called a meta-server overload. Let's understand how that happens. Let's say that there's a whole bunch of users that are generating content, and one guy generates a content with a key of 149, another guy Generates a content with a key 148, 152, 153 and so on and now what they want to do is they want to place this key value pair this guy's node ID is 80. This guy's node ID is something else. Something else something else and all these guys want to keep the key value pair and what will they do in terms of this greedy algorithm well they'll try to find a node whose id is closest to the key that they want to place. Well it turns out that 150 is the closest. ID corresponding to all of these keys that we're talking about here, so all of these puts will result in going to one node, namely the node ID 150, in terms of the put operation. And, first of all there's going to be congestion here if a lot of. Content hashed to a key that are so closely together they all end up in the same node ID 150. Remember that the actual content is with the putter of this key value pair, namely node 80. And, similarly, the content Is with this putter, content with this putter. What this guy is storing is the metadata that allows everybody to discover the content provider. So not only is this node going to find a lot of traffic if lots of keys map to its node ID, also the nodes that are adjacent to this node In the overlay network, they are going to be affected also. Everybody is trying to reach this guy so you can see that as we go towards this, there's sort of a presaturation effect that's going on. The tree is rooted at this destination node, and the nodes that are in close proximity, the intermediate nodes in the overlay network space They get congested, and so on. And that is what is sometimes referred to as a desaturation problem. Now this is for the putters. Further, if my content whose key value is toward here, the video that I generated from my India trip. If that gets hot, and everybody wants to Find out how to achieve that particular video 149. They're all going to make get calls. And these get calls again end up with this same metadata server, because that's the guy that is storing my key value pair 149, 80. And so all these gets saying that I want to get 149, I want to get 149, I want to get 149. All of them. Go to the same node ID 150. Again there is congestion at the meta data so it can happen either because some content is so popular that everybody that wants to discover the content provider they have only the key and they have to go to the metadata server to find out the content provider So that results in this congestion, which is the metadata server overload problem. And the combination of these puts and gets result in what I called the tree saturation problem, the tree being rooted at the congested node and affecting all the nearby nodes In the overlay network because they are the gateway to get this node that contains this particular key value pair.
</li> 
</ul>

<h2>10. Origin Server Overload</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/31.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>But the problem doesn't stop there. Metadata is overloaded, but my video has gotten popular. And so all of these guys want to download the content from me, which means that everybody is coming to me, and I have an itsy bitsy server in my basement. And that is going to get inundated with all of these download requests from all of these different clients that want to get this video from me. So, there is a metadata overload and there is a content overload that is coming about because of the fact that something became really hot and everybody wants to get that content. And this is what is called the origin server overload. </li> 
</ul>    
    
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/32.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>In general, there are two solutions to the origin server overload problem.</li> 
  <li>The first solution is to have a web proxy. And you might be familiar with this already because every organization tends to have a web proxy so that they can limit the amount of requests that have to go out of an organization. And that way if users of an organization are trying to access popular content from the Internet, the local web proxy that's available in the organization can directly serve that content to the requesters. But it turns out that the web proxy solution is not good enough for what is called a slashdot effect. And that is, if there is a breaking news and everyone wants to get the content, they will have to get the fresh content. A proxy's not good enough for that. Let's say that there's a Super Bowl going on and everybody wants to watch the Super Bowl, or at least get the latest updates from that. In that case, the web proxy's not going to be good, because the content in the web proxy is cached content. You want the live content. And the live content is only available at the origin server, not at the proxies, and therefore the origin server will get overloaded when we have such dynamic content. Content that is resulting from either breaking news or live programming, and so on.</li> 
  <li>This is where the content distribution networks come into play. The idea is that the content is automatically mirrored from the origin server at selected geographical locations. And those locations are constantly getting updated from the origin server. So that going to any of the mirrored content is the same as going to the origin server. And then what happens is that, in the content distribution network, depending on the geographical area from which a particular user request originates, the user request is dynamically re-routed to the geo-local mirror of the content so that the origin server need not be overloaded. You may be familiar with companies like Akamai which have come about for the purposes of providing content distribution network for popular content providers, like CNN or broadcast television channels like CBS, NBC, and so on. And what these organizations, that is the content providers do is get into an agreement with a content distribution network provider like Akamai, so that they don't have to worry about origin server overload because the content distribution network provider like Akamai have the solution for taking the content of the origin and automatically mirroring it in geo-local sites so that the origin server never gets overloaded. So, this is good if you are a big organization like CNN, or CBS. You can afford to pay the content distribution network provider to do this automatic mirroring. </li> 
  <li>But how can we democratize content distribution? How am I in my basement, generating a piece of video and I want to share it with the world, how am I going to make that available to everybody? If it gets hot, I don't want to get overloaded. I want the content distribution network to work without having to pay big bucks to a content distribution network provider like Akamai. And this is where the Coral System comes in. This is a paper that I've assigned you to read which has a technique for democratizing content distribution. And the details of the Coral System, and what we're going to look at next. So the Coral System addresses two issues. The first issue is the fact that if I generate some content, and I want to store the key value pair associated with the content in a DHT, I have to have a scalable way of doing it without saturating any particular node which can serve as a metadata server. So we want to avoid the tree-saturation effect and the second thing we want to do is also avoid the origin server overload. So both of those things are being addressed in providing a democratic solution for content distribution for the average Joe that may want to generate some content, and want to share it with the rest of the world. We will look at the details of the Coral System in the rest of this lecture.
</li> 
</ul>
    
    
<h2>11. Greedy Approach Leads to Tree Saturation</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/33.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>We mentioned that the greedy approach of constructing a DHT leads to tree saturation. And the congestion happens at the node, which happens to map to a lot of clustered keys. The coral approach is very simple, don't be greedy. What does that mean? well what it mean the greedy approach,when we have a key K, we try to store it in the lower N who's note ID is equal to K the coral approach use this as a hint not and absolute. But you still have to have a method to the madness, if you're going to store it in some place different from N. Then, those who are trying to discover it, have to have a method to the madness of where you stored your key. We'll see how that is done.</li> 
</ul>    
    
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/34.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul> 
  <li> So the top level bit I want you to take away is that in the Coral DHT The get and put operations are satisfied by nodes that are different from the key K. The node IDN may not be close to key, and that's why the DHT that Coral implements is called a sloppy DHT, and we'll talk about details of that in a minute. The rationale of course, is you want to avoid tree saturation that comes about when lot of keys map to a particular node ID. Also in the process, what we want to do is, spread the metadata overload so that no single node in this democratic process of helping one another is saturated or overloaded by being a good citizen.</li> 
  <li>How does Coral do it? It has a novel key-based routing algorithm, which we'll describe, but the basis for that key-based routing algorithm is to compute the distance between the source of a get or a put operation and the destination for that get or put operation. The destination by default is going to be the node whose ID is equal to the key that you're looking for. So the distance between the source and the destination is a XOR distance, meaning you do XOR the bit pattern of the node ID for the source and the bit pattern for the node ID for the destination that X or of the two qualities give you the distance between the two nodes in the overlay network space. And the reason why you want to do an XOR is because an XOR operation as opposed to, say, doing a subtraction, is going to be much faster. We do an XOR to find out the distance between the source and the destination that need to communicate. So, for instance, if the source at, this is fourteen, and the destination address is four. Remember that these are null IDs in the null ID space which at the level of the user. And if you do an X sort of that you get 10, so 10 is the distance between the source 14 and,and the destination for. The bigger the XOR value, the larger the distance between the source and the destination in the application namespace. And since we are dealing with fairly big numbers here, the node ID could be 160 bit quantity. And that is the reason we want to use some simple operation that will get us the distance. XOR is a very quick operation to implement to get the distance between source and destination.
</li> 
</ul>   
    
<h2>12. Key Based Routing</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/35.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Let's discuss Key Based Routing. First, we'll talk about the greedy approach. What I'm showing you here is the state of the routing table at the source node, which is fourteen. And the key that I'm looking for, let's say is four, and therefore in the greedy approach, I would assume that The key K, that is K being four, will be stored at a destination whose node ID is four. And what this table is showing, is the routing table at the source, and in particular, the entry that you have here, where you have a valid entry, those are the nodes that are reachable from. Node fourteen. So from the source node fourteen I know how to reach node number thirteen, I know how to reach node number three, I know how to reach node number two, zero and five. I don't know yet how to reach node number four because that is not in my routing table at this point of time. The intuition in the greedy approach is if I'm looking for a particular key, K, in this case, K is 4, then I know that the node that is likely to have that key, K, in this case 4, is going to be the node whose ID is also 4 and so this is my desired destination, but what I'm going to do in the greedy approach is get as close to the desired destination in the node_id namespace. If I look at the state of my routing table, I don't know how to get to 4, which is my desired destination, but I know how to get to 5 which is close to The desired destination in the name space of the node ID's, so what I am going to do, is ask this guy, because I know how to reach him, and he is close to my desired destination, do you have a way of getting to destination number 4? And my hope is that he may know the route to get me to the desired destination, and I'll be done. And getting the key value pair that I'm looking for. That's the idea in greedy routing. Take the minimum number of hops to get to the desired destination by using information that is available in my user level routing table, that has information about which are the nodes that are reachable from me directly. So the objective in the greedy approach to routing is reaching the destination with the fewest number of hops. That's the key. So in other words I am optimizing my own look up. This me first approach we know can lead to congestion and particularly the tree saturation that I mentioned earlier.
</li> 
</ul>    
    
<h2>13. Coral Key Based Routing</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/36.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>So the Coral key-based routing takes a different approach. In the Coral key-based routing, rather than being greedy, we are going to slowly progress towards our desired destination. In particular, I mentioned that the distance between two nodes in the node namespace is given by the XOR of the node IDs. So, in particular, if I look at the source 14 here, and the destination four here, we can compute the XOR distance between the source and the desired destination. And the routing table now is populated with numbers. And what these numbers show is the XOR distance from any particular node to the desired destination. So, the XOR distance from my source, which is 14, to the destination, which is four, is ten. The XOR distance from the node whose ID is 13 and the destination is four is nine. And similarly, the XOR distance from the node whose ID is five and the destination four, is one. So what I have now in my routing table is the XOR distance of the desired destination. That's what I have in this entry of this routing table that says what is the XOR distance for my desired destination right now from each of the guys that I know how to get to directly? I know how to get to 13. I know how to get to three, and two, and zero, and five. And what I am looking at now is if I get to 13, what is the distance of that guy from the desired destination? What is the distance of this guy from the desired destination and so on? That's what these table entries are showing right now. These are the nodes that are directly reachable from me. And what they showing is the XOR distance of each of the nodes that are directly reachable from me to the desired destination. So in Coral, what we are going to do is in each hop I'm going to go to some node that is half the distance to the destination in the node ID namespace. Recall in the greedy approach since I have a way of getting to node number five, I directly went to him with the hope that he'll get me to my desired destination. Not so in Coral key-based routing. What we're going to do is in each hop we're going to go to some node that is half the distance to the destination in the node ID namespace. Now the XOR distance between the source and the destination, 14 and 4, is ten. So in the first hop I'll go to the node that is half the distance to my desired destination, desired destination being ten. I want to go to a node, number five. Second hop, I want to go to a node that is half the distance of five. That is two distant from the desired destination. And third hop I want to go to a node that is one distant. That is, finally I am home. So that's the idea behind the Coral key-based routing. But of course, when I have a particular distance metric, like ten in this case, and I want to go half the distance in the first hop I may not have a direct way of reaching the guy who is half the distance to the desired node. So let's see how actually Coral key-based routing works, given this particular example.
</li> 
</ul>    
    
<h2>14. Key Based Routing in Coral</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/37.JPG?raw=true" alt="drawing" width="700"/>
</p>


<ul>
  <li>So we look at the evolution of the routing table of this source using this coral approach to key-based routing where in every step, we are going the half distance toward the desired destination. Now reducing the distance by exactly half, may not always be possible because I may not have a way to reach that particular node. So it is approximately we reducing the distance by half, so lets run through this example to illustrate, how the there key based routers works? So the XOR distance between the source and the destination is is 10, so target for my first half is going to be,to a node that is five distant from the desired destination. The node that is five distant from the desired destination is node number one because the XOR of one and four is five. So this is my target that is half the distance to my desired destination. But unfortunately I don't have a direct way. Of reaching one because I don't have that entry in my routing table and therefore, I'm going to go to a node that is approximately half the distance and I could have gone to either two or zero, close to the desired half the distance metric, but to make forward progress towards the desired destination, I'll go to this guy who is Four distant from the desired destination. So I make a iiiicall to this node because I can not reach him. I go to him, and I'm going to ask him. Hey I am looking for someone who is two distant from my desired destination which is four. And when I send my request over to him He responds to me and says, look I have information on three nodes that are not exactly too distant but close enough, four, five and seven are the nodes that I have. Who are close enough to the two distant neighbor that you're looking for. The two distant neighbor is this guy. But even this node that I'm reaching that is four distant from my desired destination, he doesn't know anyone that is two distant from the destination, and so he is going to respond to me and say. I don't have exactly what you're looking for, but I have information about nodes four, five, and seven that are close enough to who you're looking for. That's the information that I get back. So when i get the response from this node, what I'm getting back is information on how to reach nodes number four, five, and seven. So I'm now going to populate my routing table to evolve it to this new state where it shows that in addition to what I started with originally, I have now new information about how to reach node number four Node number five, which I already had, and node number seven, which is the new information I got. So you can see that from this, I evolved to this by adding two more reachable entries in my routing table, namely nodes four and seven. So what do I want to do next? Well, the next thing that I want to do is, I want to go to someone who's too distant from the desired destination because I reduced the distance by half, and now I want to reduce it even further by half. That means I want to get to a node that is too distant from the desired destination, that is this guy right here. But I don't have a way of reaching him. So I'm going to go to a node that is close enough to the desired target. The target is too distant from. The desired destination. I don't have an entry for him. I could either go to the guy that is three distant from the desired destinatnion, or that is one distant from the desired destination. Obiviously I want to go closer to the destination so I'm going to go to this guy. Normally if I went to the guy who is Too distant from the destination I would have asked him for. I'm looking for someone who is one distant from the desired destination, but in this case, it turns out that I've already reached the guy that is one distant, because I did not have an entry for the guy who is two distant from the desired destination. So this guy is going to get my request that says I'm looking for somebody who is one distant from the desired destination and he looks at his ??? table and says, these are the nodes that I know, satisfy the criteria that you're looking for, 4, ,5, and 7. That's what I get back. I know that one thing is happening here, and that is - After my first hop, I got back node IDs for four, five, and seven, which are the response for my requests saying I'm looking for somebody who's too distant from the desired destination. Because it gave approximately the nodes that are having the characteristic of being too distant from the desired destination, including. The node number four itself. So my table evolved at this step with a direct way of reaching node number four as well. But notice that I'm not being greedy here. Because in the second step, I want to reduce my distance only by half from the first step, and that is. I want to go to somebody who's too distant from the design destination. That's what I did. And I got back this response. And when I get back the response, I'm going to populate my table again. But in turns out, I did not get any new information at this step because I only have the information about four, five and seven. At the end of my first RPC call as well. Now I have a way of reading the ??? and in my conscious I can feel good that I'm not being greedy I can now the target is zero and now I can make the call directly To the desired destination and get back the response that I want. So you can see that in this coral's key-based routing, using this idea of reducing my distance to my desired destination by half at every harp. What it results in is the fact that the latency that I am going to experience in order to reach my desired destination is increased, because I have to take more number of hops in order to get to the desired destination. Even if I have a direct way of reaching my desired destination, I am not doing that. I am actually reducing my distance by half every time. In order to get to my desired destination and the reason is I'm placing common good more important than my own latency. I'm avoiding the tree saturation that can occur at the destination if everybody is greedy. That comes at the cost of increased latency But that's okay. What we are shooting for is common good.
</li> 
</ul>    
    
<h2>15. Coral Sloppy DHT</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/38.JPG?raw=true" alt="drawing" width="700"/>
</p>


<ul>
  <li>Now let's discuss the primitives that are available in the coral for manipulating the floppy DHT in particular the put and get operation. The primitives are exactly the same, it's just that the semantics of the put and get are very different in terms of how it is actually implemented. So put takes on two parameters Key and value. Key is the content hash, and value is the node ID of the proxy with the content for the particular key. Essentially, this put is announcing the willingness of the proxy to serve the content whose signature is key. The put can be initiated by the origin server with the new content, or it could also be initiated by a node that just downloaded the content and wants to serve as a proxy to reduce the load of the origin server. In both cases they will want to do a put operation. And the result of doing the put operation is to store this key value, in some metadata server. That is some node that is going to serve as a name server that can, answer queries coming in saying, I am looking for this key. In that case, that metadata server can return the value, associated with that key. So what we need to do when we do a put operation, is to place this key value in an appropriate node. Now, what do we mean by an appropriate node? Ideally, what we want to do is, given a key, we want to store it in a node ID whose ID is N equal to key. That's the desired node where we want to place it, but we want to do this without really causing a metadata server overload. Now how do we determine if a particular node is overloaded? Well, what we're going to do is define two states. One state is called a full state, and what that is saying is a particular node, lets say node n, is already storing l values for a key. Remember what I said earlier, this key value pair can be placed by either the origin server that is creating the content. Or it could be placed by a proxy who is saying, I'm willing to serve as a proxy for the content. So there could be lots of nodes that have the content, and are willing to serve the content. All of them would have done put operations. So the node that matches exactly the key. That, is being put. They already have quite a few candidates, that are willing to serve as the content providers for that particular key. That's this parameter full that's saying, I'm willing to host up to L entries, for this key value pair. Anything more than that, I'm not going to do that. I'll get overloaded. So the full is a condition, you could say it's a special condition that's saying, I'm willing to entertain up to l content providers for a particular key k. The second way a particular node may get overloaded is, if it actually starts getting a large number of requests for a particular key. So, this is a time metric that says a node has a beta parameter, and the beta parameter is the number of requests per unit time. That a node is entertaining. So if it says that I'm already entertaining beta requests for this particular key and therefore, if you want me to store the same key, I'm going to say no, I cannot do it because I'll get overloaded. So this is a space metric that's saying, how many values I'm willing to store for a particular key. Loaded is stating how many requests per unit time I'm willing to entertain for a particular key. Those are the two metrics we're going to use in determining whether to place a key value pair at a particular destination node.
</li> 
</ul>    
    
<h2>16. Coral Sloppy DHT (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/39.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>So let's say I'm a proxy, and I want to put a key value pair in the coral DHT. I'm going to use the key-based routing algorithm to place the key at an appropriate node. What does that mean? Well, I'm going to take the key, and even if I have the node ID that is equal to this key, I'm not going to go to him directly. Remember that, the Coral, key based routing algorithm reducing the distance by half. So I'm going to go to a node that is half the distance to the desired destination. So the desired destination is n. I'll go to a node that is half the distance, say n over two, n over four, and so on, til I get to the desired destination. And as I'm going, making these calls, saying that, well I'm progressing towards this desired destination node, using that key base routing algorithm that uses the XR distance between the source of the destination halving the distance at every step. What I'm going to ask is, are you loaded or full? These two states that we talked about. And this guy says, no I'm not loaded or full, and here is the next hop you can go to. So I keep going for, forward. And if none of these guys say that they are loaded or full, I would eventually reach my desired destination and place the key value over there. However, when I do this hop, going from hazardous to one fourth of this and so on, somebody along the way may say that, look for this particular key that you're trying to place. I'm already full, all loaded, one of those two conditions is already applying to me. So this node responds back to me and says, I am either loaded, or full. If that is the response I get back, then what I'm going to infer from the response is that the rest of the distance going toward the destination is all clogged up, because of the tree saturation. And therefore, I'm not going to even try to place the key value pair at the destination, at the desire destination. And not even at this guy because he's also loaded and or full for the same key. And so I'm going to retract my step and choose this node as the node to place the key value pair. So in other words, when I do a put operation, there are two phases to it. The first phase is the forward phase. In the forward phase, what we are doing is, we are going to the guy that is half the distance to the desired destination asking him are you full or loaded. He says, no, I am not. Then you go to the next guy who is even closer to the desired destination. He says, he's not full or loaded. Then you go to the next guy, who is even closer. Using that, key based routing algorithm that I described to you earlier. So keep going till you hit a node, that is either loaded or full. At that point you know, you don't want to go any further towards the desired destination because all of these guys are either loaded or full, dealing with this particular key. And therefore, the second phase of the algorithm that I'm going to use, is retract my steps and go back, and ensure that, that this guy is still willing to host my key value pair. Why would he change his mind? Well, between the time that that I am making this forward motion it is possible that this guy got either full or loaded. So I'll recheck the condition. It says still I'm good to host your key value pair, I'll choose this node for the Put operation. That's how the Coral Put operation works. So you can see that we are not storing the key in the desired destination, which should have been the way a greedy algorithm would have worked. But in the sloppy algorithm of Coral, we choose an appropriate node that is neither full nor loaded, so that it can entertain requests for retrieving this particular key value pair. So we've avoided the meta server overload by doing this key-based routing in the forward path during the put operation. So the get operation is going to do exactly similarly. That is given a key that I'm looking for, I'm not going to go directly to the destination that might be hosting it, as would happen in the greedy approach. But instead, what I would do is go to a node that it is half the distance to the key I'm looking for. And when I do that, my hope is I'll find the key somewhere along the way. Because some guy may be serving as the meta data server for that particular key. If not, I will go, to the, destination. If nobody has retrieved that key before, it will be available at the desired destination. I'll get it from there. But the hope is that, if, a content is popular enough, then, multiple people, multiple proxies may have gotten the key value pair. And therefore, and they may have gotten the key value pair, and in turn when they have gotten the content as well, they will have put their own node IDs as a potential node for the content. And so, our metadata server, when we are looking for a particular key may not necessarily have to be the destination which exactly matches that key.
</li> 
</ul>    
    
<h2>17. Coral in Action</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/40.JPG?raw=true" alt="drawing" width="700"/>
</p>
<!-- <ul>
  <li>So now let's see Coral in action for putting and getting content, user-generated content, that can be distributed in a democratic fashion and the load for both serving as a metadata server as well as the content server can get naturally distributed. Because of the way the Coral system works in managing the sloppy DHT. Let's say, Naomi, who is at node number 30, has some interesting content, and she wants to share it with the world. So what she does, she creates a unique signature, a key, for this content by hashing it. And let's say that the key that she generated for this content is 100. So now, Naomi wants to put the key, 100, and the value, 30, indicating that this node has the content corresponding to this key, 100. She wants to put it out on the internet, and so she uses the Coral system. And she uses the Coral key-based routing. The node that she would like to store this key 100 is node 100 corresponding to David's computer, okay? Because David's computer has node ID 100 so that's the place I would like to keep it, but we are following the Corals key-based routing algorithm. So Naomi, what she's going to do is going to make a series of RPC calls to put 100, 30 key value pair. And she finds that none of the intermediate nodes are either full or loaded. And finally, she reaches David's computer. David also says my node is neither full nor loaded. How can it be, because she just created this content, 100. So this 100, key 100, is not known to the world. So nobody is at this point serving as a metadata server for this particular key. So David is the right place to keep it, so David hosts this particular key value pair, 100, 30. Jacques finds out that there is this interesting video whose signature is 100, so he wants to get it. And he knows that the likely place where it is contained is node number 100, but once again, he is going to use the Coral key-based routing, and he is doing a get call, and the get call follows the same key-based routing algorithm of halving the distance to the destination. And so we make a whole bunch of RPC calls, finally get to the destination itself because none of the intermediate nodes have this key value pair. So we get to David's computer, and David says, yes, I do have the key value pair 100, 30, and here is the value that you are looking for associated with the key that you are asking about, and the value is 30 indicating that 30 is the node that has the content that corresponds to this key 100. That's what Jacques is going to get back. So then Jacques gets his response from, from David that the value is 30, that value indicates the node ID from which Jacques can download the content corresponding to the key 100. That's Naomi's computer. So, Jacques goes to Naomi's computer and gets the content corresponding to key 100. Naomi sends the content, so Jacques is now happy. He's got the content that corresponds to 100. But Jacques is a nice guy too. So he says well, I have the content. Since I have the content, I can also serve as a proxy for Naomi. And what I'm going to do, is I'm going to put the key value pair 100 corresponding to this content that is now mirrored over here. And say that the value is 60 indicating that I'm willing to serve as a proxy for the same content. So I'm going to do a pull operation, and this pull operation is going to go down, and this pull operation is going to use the same key-based routing algorithm. And when it gets to David, David might say look I am not interested in holding more than one value for this particular key. And so if he says that I don't want to do it then I have to retract my steps and pick an intermediate node which said that it is willing. Because it is neither loaded nor full for this particular key 100, so it's willing to serve as a metadata server. We already have one metadata server, but this guy is willing to do that for only one value, and therefore this guy becomes a new metadata server for the same key 100. So in this metadata server, new metadata server, we've got this entry 100, 60 also stored. So now there are two metadata servers that can potentially answer queries that concern this video 100. Now if a third guy, Kamal, comes to know about this cool video that is now propagating on the internet and he finds that the key for that is 100. He can once again query the Coral system for that video and he is following the same key-based routing algorithm of Coral, and trying to get towards David's node, which is node 100. So he's going to follow that, but when he does that he hits this intermediate node and this guy says, you know what, I've got the key that you're looking for and the value that is associated with this key is 60. So Kamal doesn't have to go all the way to this metadata server. He can get the answer for his query, get 100, from this intermediate node itself which returns a different value. Different from Naomi's address. Namely node ID 60 that corresponds to the new good Samaritan, Jacques, who's also willing to serve as a proxy for the same video content. So 60 gets return to Kamal and Kamal can then go to Jacques and get the content from, from Jacques. And that way you see that the origin of this particular video, which started at Naomi, is now propagated to Jacques. So the origin server need not get overloaded and of course Kamal will turn around and become a good Samaritan himself and say that he is willing to serve as a proxy also. So his key value pair entry gets into another intermediate node. Now we've got three nodes that can serve as metadata server for this particular key 100. Not just the original metadata server node, David's computer, but intermediate nodes that also have become part of the metadata server network for this particular key 100. And similarly, there is no origin server overload also. Because now the content itself has gotten distributed in several proxies. And all of these proxies have dynamically gotten the content, and have shared their willingness to serve as proxies. So if a new node wants to get the same video 100, when it makes its get operation, that get operation is going to traverse the network, and either hit David himself or hopefully one of the intermediate metadata servers. And that way the request for the actual content may go to different content providers dynamically as the system evolves. So as a result you can see that the metadata server load is distributed. And the origin server is also not stressed. That's the nice thing about the Coral sloppy DHT approach. So the key takeaway in the Coral approach, is that even though an individual request may have a little bit more latency because we're not trying to reach the desired destination directly, but going through some intermediate hops. In particular halving the distance to the desired destination. You're going to increase the latency a little bit, but we are doing that in the common good that in these kinds of environments, giant scale services, big data, large numbers of users, and content suddenly becoming popular, all of this dynamism has be dealt with in a system that is as vibrant as the internet. And Coral is a step towards that by reducing the stress on the origin server, as well as reducing the stress on the metaservers by naturally distributing it.
</li> 
</ul>    -->
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/41.JPG?raw=true" alt="drawing" width="700"/>
</p>
<ul>
  <li></li> 
</ul>       
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/42.JPG?raw=true" alt="drawing" width="700"/>
</p>
<ul>
  <li></li> 
</ul>       
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/43.JPG?raw=true" alt="drawing" width="700"/>
</p>
<ul>
  <li></li> 
</ul>       
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/44.JPG?raw=true" alt="drawing" width="700"/>
</p>
<ul>
  <li></li> 
</ul>   
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/45.JPG?raw=true" alt="drawing" width="700"/>
</p>
<ul>
  <li></li> 
</ul>   
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/46.JPG?raw=true" alt="drawing" width="700"/>
</p>
    
<ul>
  <li></li> 
</ul>    
    
<h2>18. Content Delivery Networks Conclusion</h2>
<ul>
  <li>We covered a lot of ground in this lesson spanning DHTs, CDNs, key based routing and how to avoid overloading the matter data server and the origin server using the concept of sloppy DHT. What Coral System does is offer a vehicle to democratize content generation, storage, and distribution, through a participatory approach. I encourage you to read the paper in full, to understand how the system has been implemented. Of course, Commerical CDNs such as Acmite, do not operate in this way. They're in it for the money. They contractually mirror the content for a customer. And they deploy their own additional mirrors to deal with dynamic increase in volume of requests. With this discussion, we complete the lesson module on Internet Scale Computing.
</li> 
</ul>    
        
<!-- <h2></h2>

<p align="center">
   <img src="" alt="drawing" width="500"/>
</p>

<ul>
  <li></li> 
  <li></li> 
  <li></li> 

</ul> -->
