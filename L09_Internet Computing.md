# Lesson outline
- [L09a: Giant Scale Services](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L09_Internet%20Computing.md#l09a-giant-scale-services)
- [L09b: MapReduce](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L09_Internet%20Computing.md#l09b-mapreduce)
- [L09c: Content Delivery Networks](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L09_Internet%20Computing.md#l09c-content-delivery-networks)

# L09a: Giant Scale Services
<h2>1. Giant Scale Services Introduction</h2>
<ul>
  <li>What are the systems issues in managing large data centers? How do you program big data applications, such as search engines to run on massively large clusters? How do you store and disseminate content on the web in a scalable manner? These are the kinds of the questions we will be addressing in this module of the course. This is what we mean when we use the term internet scale computing. A comforting factor for you is that you already know the answer to these questions. We've learned through the previous lessons, most of the techniques needed in constructing distributed services. What we're going to see in the upcoming lesson is a hardening of the distributed systems issues to handle scale on the order of thousands of processes and failures. It's not a question of if. It's a question of when a failure's going to happen.</li> 
</ul>

<h2>2. Giant Scale Services</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/1.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/2.JPG?raw=true" alt="drawing" width="500"/>
</p>

<h2>3. Tablet Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/3.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>In this module, we will cover three topics. The first is systems issues in providing giant scale services on the internet. And the second topic is what exactly is a programming model that is used for the applications that are delivering the service that you're expecting on an every day basis, so-called Big Data applications. And the third topic is content distribution networks that are used for the dissemination of the content when you access giant scale services on the internet. So in this lesson, we are focusing on what are the issues in giant scale services from the perspective of the system.</li> 
</ul>

<h2>4. Generic Service Model of Giant Scale Services</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/4.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Imagine you're going on the Internet and reaching a web portal, such as gmail, let's say, for accessing your email. In fact, while you're doing this, a million other clients are doing exactly the same thing trying to reach the web portal through the IP network. The service that you're trying to reach may be architected to direct your particular request to a specific site. And the architecture within that site may look like this, a whole bunch of servers, maybe on the order of thousands or even 10,000 within a cold dark room, interconnected to one another through perhaps a high bandwidth communication backplane. And also all of the servers are connected to data stores for processing requests that may be coming in. And the servers may optionally use a backplane that allows them to talk to one another for servicing any particular request. Any of the servers may be able to handle an incoming client request.</li> 
  <li>Now in the generic picture that I'm showing you here, the load manager that is sitting in between these servers, and the IP network redirects an incoming client's request to one the servers. The chosen server may than process the query and answer the client. The role of the load manager is to balance the client traffic among all the servers. Basically, the load manager has to make sure that no particular server is overloaded. One of the characteristics of most of the giant scale services is that the these client requests that are coming in are all independent of one another. Sometimes, this also called embarrassingly parallel, meaning that all of these client requests can be handled in parallel as long as it is enough server capacity to meet all the incoming requests. So the rule of the load manager is to balance the client traffic and redirect them to the server so that no particular server is overloaded and all of the servers get equally utilized in servicing incoming requests.</li> 
  <li>The other responsibility of the load manager is to hide partial failures. Because when we're talking about the scale of giant scale services, and the kind of computational power that resides in the data center for servicing these giant scale services, they're humongous, they're 10,000 nodes. And when you have so many nodes, compute nodes and data nodes and so on, it's not a question of if something will fail, but is a question of when something is going to fail. So the load manager has to make sure that it is observing the state of the servers and ensuring that the incoming client requests are shielded from any such failures that may happen internally within a particular site.</li> 
</ul>

<h2>5. Clusters as Workhorses</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/5.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Compuptational clusters are the workhorses of giant scale services, any modern data center employs on the order of thousands and thousands of computational nodes connected by high-performance high-speed networks. It is such computational clusters, which are serving as the workhorses for giant scale services today. And this figure reproduced from Eric A. Brewer's paper circa 2,000 shows the number of nodes in a data center, maybe on the order of 1,000. But scale it up by ten or 100 in terms of today's computation capacity that you might find in a typical data center. And the kind of volume of queries that are going to be handled by giant scale services today.</li> 
  <li>Each node in the cluster may itself be an SMP. There are serveral virtues for structuring the computational resources inside a data center as a cluster of machines connected by high performance backplanes. The advantages include absolute scalability. You can keep adding more resources without worrying about re-architecting the internals of the data center. Cost and performance is another important issue and that is by having computational clusters at every node is identical to others, is that as a system administrator we can control both the cost and the performance of running a data center like that. Because of the independent components you can easily mix and match generational changes in the hardware, and the results are incremental scaleability you add more resources you get more in terms of performance because, as I told you earlier, most of the queries that come into giant-scale services tend to be embarrassingly parallel. So the more resources you've got, the more queries you can handle. There is incremental scalability when you're using clusters because you can add more nodes to the cluster, increasing the performance. Or, if the volume of request come down, you can scale back. That's the advantage of using computational clusters as the workhorses for data centers that are serving giant scale services today.</li> 
</ul>

<h2>6. Load Management Choices</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/6.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>As a refresher, I'm showing you the seven layer OSI reference model of the protocol stack. Now, load management, as I mentioned, for dealing with client requests, can be done at any of the levels of the seven layer OSI model from the network layer and up. And higher the layer, you have more functionality that you can associate with the load manager in the terms of how it deals with the server failures, how it deals with directing incoming client requests to the different servers, all of these are possible. The higher the layer at which you construct the load manager.
</li> 
</ul>

<h2>7. Load Management at Network Level</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/7.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>The load manager may be operating at the network level, that is, at the network layer level of the seven layer OSI reference model. In this case, the load manager is simply a round robin domain name server. What it does is, when client request comes in, they all have the same domain name in order to come to this particular server. If you say gmail.com, then depending on how the server is architected, the gmail request coming from particular client may go to a particular site. And when it goes to that site, the domain name server, which is acting as a load manager, it is going to direct the incoming client request to one of the servers. So it is giving different IP addresses. Even though the domain name a client is coming in is exactly the same, the round robin DNS server assigns different IP addresses corresponding to different servers to the incoming client request. And that way these clients are being redirected to different servers, and because the load manager is doing it at the level of individual server addresses, you get very good load balance. And the the inherent assumption in this model, is that all the servers are identical, so an incoming request can be sent to any one of these servers, and perhaps the model is also that the data is fully replicated. So that if an incoming request goes to this server, it has access to all the data. If it goes to this server, it has access to all the data to satisfy the incoming client request.</li> 
  <li>The pro is good load balance because the Round Robin DNS scheduler can choose the least loaded server to redirect an incoming client to that particular server.</li> 
  <li>The con is, it cannot hide down server nodes, because it is assigning IP addresses of the server to the incoming request, the load manager is not able to hide down server nodes from the external world. </li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/8.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So now we move the load manager up in the OSI reference model, up to the transport level or even higher. The load manager could be layer four switches. That is transport level switches or even higher level switches. In fact, the layer four switches may be architected as switch pairs so that there is hot failover from one fail switch to another fail switch at the load balancer level. When you do this architectecting the load balancer at the transport or higher levels. It gives you the opportunity to dynamically isolate down server nodes from the external world, and it is also possible with this architecture of load manager to have service specific front end nodes. For example, instead of dealing with arbitrary client requests, we're actually dealing with requests based on the kind of requests coming in from a particular client. For instance, this could be a Gmail client. This could be a Google search client. This could be for a photo server like Picasa. So those are different functionalities that can be implemented at the frontend of this load manager to look at the service, the particular program, that is actually triggering a client request coming in to this particular site, and make decisions as to who should serve that particular request. So in other words, we're building more intelligence into the load manager. So that it can know the semantics of the client server communication that is going to happen based on the type of request that is coming in.</li> 
  <li>It is also possible with this structure of having the load management at higher levels of the OSI reference model to co-opt the client devices in the load management. For example, it's possible that the load manager may even know the device characteristics that the client is using, in order to come into the site. For example, if it's a smart phone, it might take certain actions commensurate with the device that is coming in, in terms of structuring the client-server interactions. Now, the data itself may be partitioned among all the servers. For example, let's say that you are doing a web search and it is a web search that came in as a clients request. And, if it is redirected to a particular server, this server may not have access to all the data that it needs. In order to do that, Google search that you came in with. In that case, the server may communicate using the backplane with other servers to get the data that it needs to serve the queries that is coming in, so that there is complete coverage of whatever the intended query needs to get as a result. Of course, if the data is partitioned, if a server that is holding a portion of the data is down, then there'll be data loss for the query. So for your specific search, the server that is handling your search may not be able to give you complete result for the search, because a portion of the data is down due to the partitioning of the data among the different servers at this site. Now, this is where replicating the servers for redundancy helps. So even though you may partition it, you may replicate partitions so that it is possible that even if one server is down, another server that has a replica of the same partition may be able to provide the data that is needed in order to satisfy a particular query.</li> 
</ul>


<h2>8. DQ Principle</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/9.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Thus far, I've given you a general idea of how an individual site that hosts computation resources and handles a whole bunch of incoming client requests may handle these incoming client requests in terms of load management. This brings us to the DQ principle.</li> 
  <li>The server has all the data that is required for dealing with incoming client queries. So this the full data set, or Df if the full data set, that is required for handling any incoming queries to the server. Now clients come in with their queries and we will call Q0 as the offered load to the server, meaning this is the amount of requests that are hitting the server per unit time. Even though the offered load is still Q0, maybe the server could only manage to serve a portion of this incoming requests. And that portion of the incoming requests that is completed by the server is Qc. These are the completed requests. We define the yield Q as the ratio: Qc over Qo. That is the ratio of completed requests to the offered load. So, Q is going to be a fraction between 0 and 1. Ideally you want it to be one so that all the client requests are serviced, but if the server is not able to deal with the offered load entirely, then the yield is going to be less than one.</li> 
  <li>Now each query that is coming into the server May require processing all of the data that's available in the server to answer this particular query. For instance, if there's a Google search and the search requires looking at all the corpus of data that the server has to answer the query, then you want to look at the full data set Df. However, it may be that because of either failures of some of the data servers or the load on the server, it is only able to process the query using a portion of the total data set, Dv. Dv is the available data. That is used in processing the query, in that case the harvest D is defined as the ratio Dv over Df. That is harvest is defined as the ratio of the available data to the full corpus of data. Again, this is going to be a ratio between 0 and 1. Ideally, you want to make sure that the harvest is one, meaning that incoming requests is completely served with all the data that it wants. Depending on the service I can give you different examples. So for instance if the incoming request is a search, you want to look at the whole corpus of data that is available to you to answer that search. But if you are not able to do that and the server is only using a portion of the full data in the harvest in terms of the quality of the results that is being given out to a particular client is less than one.
</li> 
</ul>

<h2>9. DQ Principle (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/10.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The product DQ, that is the data server query and the rate of query that is coming into the server. This DQ represents some sort of system limit for a given server capacity the product DQ is a constant. Said differently for a different system capacity, we can increase the number of clients that we are serving if we reduce the amount of data that we're using to process the incoming queries. That is, we're increasing Q by decreasing the harvest or D. Or we can do the opposite and that is, we can entertain a smaller set of queries. That is the yield will come down but we will be able to give the complete data that is needed for serving that query. An example of would be, if I am a client that is accessing a server for my mail, I would want to get all my mail, not part of the mail. So in that case, I will be happy if the server gives me a harvest of one, even it means that not all the incoming clients maybe served by the server due to the capacity limitation of the server at any point of time.</li> 
  <li>So as a system administrator, we have some choices to play with. The important thing is that in giant's scale services. The performance is limited by network and not by the I/O capacity. We can have as much I/O capacity as we want by having more hardware resources here, but what we are bound by is the network capacity. So if we increase the capacity of a server, we have a choice as a system administrator. We can either increase the harvest, keeping the yield the same, or we can increase the number of clients we are serving, that is, we can increase the yield keeping D a constant. That's the knob that a system administrator can play with in terms of dealing with capacity increases at the server.</li> 
  <li>Similarly, if I am a system administrator, and some nodes in the server fail, then again we have a choice of either decreasing the harvest or decreasing the yield in order to deal with reduced DQ value. It all depends on how the server itself is architected, if the failures can be hidden through replication and so on. But the important message I want to convey to you is that this DQ represents a system constant, so far as the server is concerned, in terms of the capacity. Given a particular capacity of the server, DQ is fixed. So as a system administrator you have a choice of either sacrificing yield for harvest or harvest for yield. You cannot increase both the harvest and yield without increasing the server capacity.</li> 
  <li>At traditional measure that has been used in servers, is the notion of IOOPS, or I/O operations per second, and these are meaningful in database kinds of applications where the applications are disbound, but the giant scale applications are network bound, and for network bound applications this manager DQ is much more intuitive as to what is going on in terms of managing the incoming load to the server and managing the corpus of data that the server has to look at in order to satisfy incoming queries.</li> 
  <li>Another metric that system administrators have often used is what is called uptime. You may have heard of terminologies like mean-time-between-failure(MTBF) and mean-time-to-repair(MTTR). Mean-time-between-failure is what is the expected time between two successive failures inside a data center. And similarly mean-time-to-repair is if a failure is detected, how much time does it take to bring up a failed server that need time to repair. And the uptime is defined as the ratio of the difference between mean-time-between-failure(MTBF) and mean-time-to-repair (MTTR). Uptime is equal to MTBF minus MTTR divided my MTBF. That is uptime. And you want the the uptime to be close to one. And usually uptime is measured in nines. Meaning point nine, nine, nine, nine. Five nines or six nines and so on. So you want the uptime to be as close to one as possible for giant scale services. But again, this uptime as a metric is not very satisfying because if there are no queries giving the time that is needed for repairing, that is, during the MTT par, the mean-time-to-repair, if no queries come into the server, then you haven't made anybody unhappy. In that sense, the uptime is not a very intuitive measure of how well a server is performing. It is better to look at yield as the output metric. And the harvest, again, is the output metric for say how well a server is able to deal with the dynamism and scale of requesting handled by a particular giant-scale service. We'll see that this DQ principle is very powerful in advising the system administrator on how to architect the system. How much to replicate? How much to partition in terms of the data set that the server is handling? How to deal with failures? How to gracefully degrade the servers when the volume of incoming traffic increases beyond a server capacity? All of these are questions that can be handled very elegantly using the DQ principle. And the key underlying assumption in the DQ principle is that the giant scaled services are network bound, and not I/O bound.
</li> 
</ul>

<h2>10. Replication vs Partitioning</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/11.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>In architecting the data store and the computational resources that back the data store at a server, the system administrator has a choice of either replicating or partitioning the data. If you replicating the data what that means is, that every data server has the full corpus of data that is needed to serve a particular request. Think of it as say, gmail. If it is gmail, then if a gmail request gets sent to anyone of these servers. They can handle the request that comes in because they have the full corpus of data to deal with a incoming request. We mentioned that failures are inevitable in giant scale services because of the number of computation elements that live within a data center. If failures occur, what happens when we have replicated data? What they means is that the harvest is going to be unchanged because if some data repository is failed. Because it is replicated, we can redirect a client's request to another live server that has full access to the later repository and therefore the harvest that you're going to get with replication in 100%. It is unaffected. On the other hand because the server capacity has come down due to these failures, the yield is going to come down. In other words, with replication. The service is unavailable for some users for some amount of time. But all the users that are able to get the service, get complete harvest in term of the fidelity for the query that is returned and answered by the server.</li> 
  <li>The other alternative of course for architecting the internal data depository of the server is to partition the data. So if you partition the data, and lets say that There are end partitions of the full corpus of data. And let's say that some of the service fail, then some portion of the corpus of data becomes unavailable. If this happens, what it means is that the harvest is going to be down because some portion of the data corpus is unavailable, and therefore the harvest is going to come down, but it is possible to keep Q unchanged, so long as the computation capacity is unchanged. Then we can serve the same number of user community. It's just that the fidelity of the result may not be as good as when all of the data corpuses available. For example, if this is a server that is serving search results and if some portion of the data repository is unavailable. In that case, each query that comes in will get service, but it is going to get a subset of the query results for the search that they are doing.</li> 
  <li>So the important message that I want to convey through these pictures is that DQ is independent of whether we are replicating or we are partitioning the data. Given a certain server capacity, The DQs are constant, because we are assuming that this is cheap. This space is cheap and therefore processing incoming requests are not disk bound, but it is network bound. The only exception is if the queries Result in significant in right traffic to the disk. Now this is rare in giant scale services. But if that happens, then replication may require more DQ than partitioning. But as a first order of approximation, so long as we assume that giant scale services are serving client requests which are network bound, The DQ is independent of the strategy that is used by the system administrator for managing the data whether it is replication or partitioning. Now what this also means is that this is giving an object lesson to the system administrator. In saying that beyond a certain point, you probably want to replicate and partition. Why? Because failures are bound to happen, and a failures are bound to happen if you partition the data, then a portion of the data corpus becomes unavailable. On the other hand, if beyond a certain point Even if you have partitioned data, if you replicate it, then you're making sure that, even if one replica is down, some of the replica, of the same partition is available for serving a particular client request. And replication, beyond a certain point is important because users would normally prefer complete data or a complete harvest. Take for instance, you're accessing your email through the Internet. You would rather put up with a service being unavailable for some amount of time rather than seeing that you only have access to a portion of your main box. And that's the reason why replication beyond a certain point is better for dealing with giant scale services. On the other hand, for searches, it may be okay to have a harvest which is not complete. So in structuring different services as a giant scale service, one has to make a decision of whether partitioning is the right approach or at what point we may want to partition and then replicate as well. As a concrete example, Inktomi which is a CDN server uses partial replication It uses full replication for email because of what I just said, and that is a user would expect complete harvest for a service like email. But on the other hand, if it is a web cache that is being implemented as giant-scale service, it is okay if it is not replicated.
</li> 
</ul>

<h2>11. Graceful Degradation</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/12.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>The DQ principle is also very useful for managing graceful degradation of service. So DQ defines to total system capacity. So if a server is saturated, meaning that we have reached the limit of the server in terms of DQ. That's a constant. DQ's a constant. And so if you reach that limit, then we have a choice of graceful degrading the service from the point of view of the client. </li> 
  <li>One possibility is we keep the harvest the same meaning that every client request that comes in has complete fidelity in terms of the answers returned by the server. So D is fixed. Q comes down because DQ is a constant. That's one option.</li> 
  <li>The other option is to keep the volume of clients that are service, that is the yield Q to be a constant, but decrease the harvest. So the fidelity of the results returned to the users is less than 100%, but we keeping more of the user community happy by serving more of them.</li> 
  <li>Because DQ is a constant, it allows us to gracefully degrade the service being provided by the server depending on the choice you want to make in terms of fidelity of the result or the yield that you want to provide the user community. In other words. The DQ principle gives us an explicit strategy for managing saturation.</li> 
  <li>So as a system administrator, when we make these decisions on which to sacrifice and which to keep constant, we know how our decisions are going to affect the harvest, how it's going to affect the yield, how it is going to affect the up time and so on. So the choices that a system provider has or strategies that a system provider can use in structuring the servers, knowing that DQ is a constant, is when the server is saturated. They can do cost based admission control. You pay more, you get more. That may be one way to do it. Or priority or value based admission control. That may be another way to deal with service saturation or reduce data freshness. That is the harvest may be the same, but you reduce the fidelity of the data. For instance, if I'm serving videos, and I can serve the videos at different bit rates, if the server is saturated, I might decide to serve the video to all the users, all the videos that they want, but at a lower bit rate. So that is reducing the fidelity of the data that is being harvested. So that's the idea behind the DQ principle, how it might be used for graceful degradation of service when the server is saturated.
</li> 
</ul>

<h2>12. Online Evolution and Growth</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/13.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>As a giant scale service provider, another thing that the service provider has to worry about is online evolution and growth of the service. because services are evolving continuously. The Gmail that you're accessing for getting your email today may not be the same that you accessed a month back. So, since the services are continuously evolving, the servers at the data centers have to be upgraded with a new version of the software or maybe new nodes are being added, or maybe all the old nodes are being retired, and brand-new nodes are being brought in to serve the user community. Whether it is hardware or software upgrade that needs to be done, that has to be done with the understanding that while that upgrade is happening, there is going to be loss of service. Again the DQ principal comes in handy in measuring what exactly is the loss that we're observing of the service when we do this kind of online evolution and growth. The first choice, is what is called fast reboot. That is, bring down all the servers at once, upgrade them, and then turn them back on. So this diagram is showing you, time on the x axis, and the loss, DQ loss, on the y axis, and what you're seeing here, this is the time for the upgrade per node. So the amount of time a node is going to be down in order to do that upgrade, whether it is a software upgrade or a hardware upgrade. And this y axis segment is the DQ loss per node. So if I bring down a node and upgrade it, it's going to be down for some amount of time. And this is the amount of server capacity that is lost, the DQ that is lost, as a result of one server being down. If all the servers are down, which is what is happening with fast reboot, then for this entire time, the area bounded by this green-shaded rectangle, we're going to have no service. The ideal is here in terms of the total amount of DQ we want, and the access is saying how much is the DQ that is lost and what we are saying is, if we do this fast reboot, that fast reboot is going to bring all the servers down for a certain amount of time that it takes to upgrade them, and for the entire duration, the service is unavailable. This idea of fast reboot is particularly useful in situations where we can use diurnal server property. For instance, if you think about services that are being all across the globe. We can use factors such as oh, this is nighttime for the user community and the users have probably gone night-night and therefore, they are not going to access the servers. This is a good time to bring down the whole service and upgrade the service. So that might be a situation where fast reboot is very useful. So we are assuming that the user community is segmented so that they're directed to geographically different locations, and we can use the diurnal server property to do this off-peak fast reboot of chosen replicas of the services. In this figure, so in this figure this segment that I'm showing you here is the DQ loss per node. And the total DQ losses, of course, n times DQ, where n is a number of servers that are undergoing this fast reboot. So in other words, for u units of time, there is complete loss of DQ with fast reboot. An alternative to fast reboot is what is called a rolling upgrade. Now here, what we're doing is, rather than bring all the servers down at the same time, we are bringing one server down at a time. And upgrading one server, bringing down the next server, upgrading it, and so on. This is what is called rolling upgrade or wave upgrade. And in this case, services available all throughout, there is no time that we are saying that the service is completely unavailable. But, for the entire duration of time, the duration of time here is n times u, because u is the upgrade time per node, and the upgrade, because it is a wave upgrade, is going to last for n times u time units, where n is a number of servers being upgraded in this fashion. But during the entire time, service is available, but in every u units of time, there's a DQ loss of this much bounded by this area during the entire upgrade process.
</li> 
</ul>

<h2>13. Online Evolution and Growth (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/14.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>A third alternative is what is called a big flip. In the big flip, what we are doing is we are bringing down half the nodes at once. So in other words, the service is available at 50% of the full capacity. So you can see that with a big flip, half the nodes are down. So we have reduced the server capacity, the total DQ available by 50% for u units of time. And the total upgrade is going to last 2 times u because we have partitioned the entire server capacity in two halves, and upgrading one half, upgrading the second half for 2 times u time units we're not going to have full capacity. And during this 2 times U time unit we get the service at 50% capacity, and at the end of this it is back up to full capacity. So these are the three different choices that you've got, the fast reboot where the upgrade time for the entire service is just equal to the upgrade time for a single node. But here, we don't have the service available for any of the users for that period of time, and this might be a, a good strategy for services that can exploit the diurnal property of user community. Rolling upgrade is the more common one, where what we are doing is we are upgrading the servers one at a time but it's going to take a long time especially when you're talking about data centers having thousands and thousands of processors. Rolling upgrade is going to take a long time and it might be done in batches for instance instead of being exactly one at a time. And the extreme of that rolling upgrade is this big flip where we are saying we'll bring down 50% of the nodes, upgrade them, and then turn them back on, and then do the upgrade for the remaining 50%. So in the third case, in the big flip, the service is always available, but at 50% capacity for a certain duration of time. But there is no way to hide the DQ loss. And you'll see that the DQ loss, which is shown by the area of this shaded rectangle in each one of these cases, the area in the shaded rectangle is exactly the same. In other words, the DQ loss is the same for all three strategies, and it is the DQ loss of an individual node, the upgrade time for an individual node, and the number of servers, n, that you have in your server farm. That's the total DQ loss for online evolution, and all that these different strategies are saying is, as a system administrator, you have a choice on how you want to dish out the DQ loss. And make it apparent and not apparent to the user community. In this case, it's going to be apparent to the entire user community that there is upgrade going on. In this case, different segments of the user community are going to notice that the service has become unavailable for a short duration of time. And here it's sort of in between these two extremes and that half the user community may see a service being unavailable for some amount of time. So, in other words, using the DQ principle, maintenance and upgrades are controlled failures that can be handled by the administrator of a system service. So what we are seeing through this concept of DQ is a way by which a system developer and a system administrator can work together in figuring out how to architect the system in terms of how the data should be partitioned or replicated and how the system should fine-tune the service by looking at the instantaneous offered load and tuning whether to keep the yield the same or the harvest the same, knowing that DQ is a constant and the server is getting saturated. And finally, when service has to evolve once again, the system administrator can make an informed decision on how to do this online evolution by controlling the DQ loss that is experienced by the user community at any point of time.
</li> 
</ul>

<h2>14. Giant Scale Services Conclusion</h2>
<ul>
  <li>The key insight is that giant scale services are network bound and not disk I/O bound. This is what helps in defining the DQ Principle. The DQ Principle really helps the system designer in optimizing either for yield or harvest for a given system capacity. Also, it helps the system designer in coming up with explicit policies for graceful degradation of services when either servers fail or load saturation happens or upgrades are planned.
</li> 
</ul>

# L09b: MapReduce
<h2>1. MapReduce Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/15.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>We saw the system's issues at a macro level from the point of view of administering the engine that powers internet scale computing. How do we program services such as websearch and airline reservation on top of the data center cluster resources? How do they exploit the hardware parallelism that's available in these clusters? We will learn about the map produced programming paradigm that answers these questions. The term Big Data, has become a buzz word. Computations in giant scale services are usually simple, but they work over large data sets and hence, the name Big Data. And naturally, because they are working with a large data set, these computations take a long time to compute. For example let's say that I want to search for John F Kennedy's photographs in all the documents that are available on the web. It's going to take a long time. And that is an example of a Big Data computation. Similar examples include online reservations, shopping, et cetera. And applications that work on Big Data would like to exploit the pluralism that's available in the cluster at the data centers. I mentioned in the last lesson that data centers these days comprise of computation elements on the order of thousands and even 10,000 nodes that are ready to do computations. And we would like to exploit the computational resources available in such Big Data centers to do computation on Big Data. For example, if it is John F Kennedy's photographs, I'm looking in all the different documents. We can parallelize that's search for Kennedy's photo in all the documents in parallel on all the available nodes in the data center. And since computations are also called embarrassingly parallel computations. What does this mean? Well, there is not much synchronization or coordination that is needed among the parallel threads that comprise the applications and that run on different nodes of the computational cluster. Looking for John F Kennedy's photographs in all the documents on the web, is a good example of such an embarrassingly parallel application. What are all the issues in programming in the large, on large data sets, so the data sets are also large and the computational resources on which we want to run this big data application is also vast. So some of the interesting issues in programming in the large include, how to parallelize an application across let's say, 10,000 machines. How to handle the data distribution and plumbing between the producers of data and consumers of data. Remember these are embarrassingly parallel applications, but different phases of the application will require data from one phase of the application to be passed on to the next phase of the application. That's what we mean by plumbing between the producers of data, intermediate data to be more specific, and consumers of that intermediate data. And of course, one of the biggest challenges in Big Data applications on large computational clusters is failure handling. Recall what we said about the nature of these data centers. They employ thousands and thousands of processes. When you have so many parts, in any setting, failure is not a question of if it will happen. It is a question of, when it is going to happen? So that's a fact of life, and therefore, programming models for Big Data, have to worry about the fact that failures are to be expected in this environment.
</li> 
</ul>

<h2>2. MapReduce</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/16.JPG?raw=true" alt="drawing" width="800"/>
</p>

<ul>
  <li>In this lesson we're going to look at one specific example of a programming environment for dealing with big data applications running on large computation clusters. And this programming environment is called map-reduce programming environment. And in this programming environment, the input to the application is considered as a set of records Identified by a key value pair. So the big data app developer, supplies the run time environment with two functions called map and reduce. And these are the only two functions that the app developer has to supply to the programming environment and both map and reduce - Take user defined key value pairs, as inputs, and produce user defined key value pairs as outputs. So both the input and the output to each of the map and reduce functions, written by, the domain expert, are key value pairs. Once again, these are key value pairs defined by the app developer. Specific to the particular application that he or she is coding. Let's consider a fun example. In this example, I'm going to say that we're looking to find specific names of individuals in a corpus of documents. Maybe the names are Kishore, Arun, Drew, and so on. These are the specific names that I want to find in a whole corpus of documents. So the input to the application is a whole corpus of documents, and we are looking for specific names and the number of times those names occur in those documents. So that's what the appllication is going to do. So the input key space for the application is the file name. And the value part of the key value bares the content of the file. So if you have n files in the corpus of documents that we want to analyze in this fashion, then you have N key value pairs corresponding to each of the different files and the respective contents. So this is going to be the input to the whole application and were going to structure this application using this programing paradigm of map reduce. And we will see how the app developer will use the map-reduce framework to accomplish the goal that we set out. Which is to find the number of occurrences of unique names, in this corpus of documents. In this example, the user defined map function is looking for each of the unique names that we're interested in the corpus of documents. So the map function will take as an input a specific file and the contents of the file as the key value pair, and emit a value that corresponds to the number of times each one of these names occur in the input file. Now, a simple version of the map function may emit a one every time it encounters the name Kishore in the input file, or the name Arun in the input, or the name Drew in the input file. The output of the map function is a key value pair, where the key is the unique name that you're looking for. One of the unique names that you're looking for, and the value is the number that says how many times it found that particular name. As I said earlier, a simple version of the map function could emit the value of 1 every time it finds one of the specified names that it is looking for in the file. Or a slightly more elaborate mapper may actually combine the number of occurrences of a particular name in the input file and then emit that as a sum of all the times a particular name occurred in this key value pair. In either case, the output of the map function is itself a key value pair. Note that it is different from the input key value pair. The output key value pair that the mapper is emitting is a unique name and a value that is associated with that unique name and from this example it should also be evident that we can spawn as many instances of the map function as the number of unique files that we have in the input document corpus. Further, since each of the map function is working on a different key-value pair as an input, they're all independent of one another and there is no need for coordination among them. And that's the feature of an embarrassingly parallel application. The output of the mappers are the input to the reducers. In other words, the output key value pair from the mapper is the same as the input key value pair for each of the reducers that you see here. Again this reduce function is something that is written by the user. And you notice that what the mapper is doing is when it notates a particular name in this example that it is looking for like Kishore then this mapper is going to send the value associated with Kishore to this reducer. Similarly this mapper is going to send the value associated corresponding to Kishore to the same reducer. And all the mappers in the same fashion are going to send the respective values that they phone for this unique name Kishore, to this reducer. Similarly, the values phone for the name Arun, is going to be sent to this video set by all the mappers, and so on. So the number of reducers that we will have is the same as the number of unique names that we are looking for in the corpus of documents. This is where work needs to be done by the programming environment to plumb the output of the mappers To the inputs of the reducers. Potentially, the mappers could be executing on different nodes of the cluster, reducers can be executing on different nodes of the cluster, and the work that is involved in this plumbing is really making sure that the outputs of the mappers are fed to the corresponding correct reducers in the entire distributed system. For example, the output that is coming out of the mapper corresponding to Kishore has to be sent to this reducer from all the mappers. The output corresponding to Arun coming from all these mappers has to be sent to this reducer, and so on. That's what we mean by the plumbing that needs to be done by the programming environment to facilitate the communication That needs to happen between instances of the mapper and the instances of the reducers. The work performed by each one of these reducers is going to be aggregation of the values that it got for each of the unique names that we're looking for in the input document corpus. So this reducer's job is to aggregate all the instances of Kishore that it got from all the mappers. So it is receiving the key-value pair corresponding to the name Kishore and the number of instances that they found in the input key-value pair that they processed, and the reducer is going to aggregate that And the output of the reducers, all the reducers, is once again going to be a key value pair. It is exactly the same as the input key value pair that the reducers got. Namely, the key is the unique name that they've been assigned to aggregate, and the value is the total number of the occurrence of this unique name. In the corpus of documents we started with. So this is the roadmap for creating a map-reduce application for a chosen application that works on big data. So in this case, the chosen application was looking for unique names in a corpus of documents, and all that the domain expert had to worry about, is deciding what is the format of the key value pair, which is the input to the whole application, in particular, the map phase of the application. And also, what is the format of the intermediate key value pair that is going to be generated by the mapper function. And what is the work that needs to be done in each of the mapper and the reducer to carry out the work that needs to be done for this particular application. Beyond that, the app developer does not have to worry about any of the other details, such as how many instances of mappers do we need to create. How do we create the number of instances of reducers corresponding to what this application is wanting to do, and also worrying about the plumbing from the output of the mappers to the input of the reducers, all of those, are things that the app developer does not have to worry about.
</li> 
</ul>

<h2>3. Why MapReduce</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/17.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>The question is why map reduce is a programming framework for big data applications. It turns out that several processing steps in giant-scale services are expressible as map reduce. For example, let's say that you're looking for seat availability for selected dates to selected destinations on different airlines. That can be expressed as a map reduce computation. Let's say you want to access frequency of the URLs on the website that you've designed. That can be quoted up as a map reduce application. Let's say you want to create. Word indexes to facilitate document searches on the web. That can be coded up as a map reduce application. Or let's say that you want to do ranking of pages. When a user is doing a search, how to present present the search results May depend on the popularity of pages and that is what is often referred to as page ranking. So if page ranking has to be done for all the documents that is another application that can be ??? up as a map-reduce application. The list goes on. All such examples that are mentioned share some common property. They're embarrassingly parallel, and they're common in giant-scale services. And all of them tend to work on big data sets. Therefore there is plenty of opportunity for taking advantage of the computation resources that are available in a data center. And all such applications need domain expertise in terms of what to do with the data. Which is expressible as a map function and a reduce function, and this is the only thing that is required of the domain expert to provide to the programming system, because that is domain expertise that lives with the app developer. So here is another example of how an application may be structured as a map_reduce application. And in this case, I'm showing you a page ranking application that is, I'm interested in knowing what is the popularity of different URLs that occur in a document corpus. So the keyspace that is input to this application is a set of URLs. And the key value pair that is coming into each of the mapper is a source URL and the contents of that web page that corresponds to this particular URL. So what this mapper is doing is, in the given page defined by this URL the contents of which is the input to this mapper it is looking for different targets. Maybe it is looking for a particular URL target one, another URL target two, and so on, target N, and that's what each of these mappers are doing. So the keyspace that is output from the mapper is unique target names. So the keyspace that is output from the mapper is unique, target URLs and, the value is the URL in which it was actually phoned. So the corpus of input URLs it is taking, and saying well, in this particular URL I phoned this target. If it did. It is emitting that this target was found in a particular URL. And this reducer is going to get all the hits for a particular target that was found in the input corpus of URLs. So all the mappers they're going to send their results to this reducer if they found in the input - Surl the target, target 1, if they did, they are going to send their results to this reducer. Similarly if they found target n, each of these mappers are going to send this reducer that in the input URL they found this target n and the job of the reducer ??? Is once again aggregation, and you have as many reducers as the number of unique targets you're trying to identify in the input dataset. Very similar to the previous application that we went over. So the output of the reducer is going to be the specific target that this guy has been asked to accumulate. And, a source list, meaning all the source pages in which this particular target was found. So each of these reduces is going to find the number of times a particular URL is found in the input corpus of webpages that came into the system as a whole. For instance, if I wanted to find out how many times my webpage appears in the universal web pages all over the world, we can take the entire corpus of web page available in the universals input, and the map that's we're going to look for occurence of my web page in each one of those input web pages. And if they find that, they're going to send it to this reducer and if target one corresponds to my web page then this reducer is going to say, okay, show his webpage, was found in this list of source webpages all over the Universe. That in a sense gives a rank for. That particular web page that we're looking at. So we're able to rank the target web pages 1 through n based on the number of source web pages that contain that particular target. And that's what page ranking is all about. So I'm giving you yet another example of how This map reduce functionality can be applied for an application such as page ranking. All the heavy lifting that needs to be done, in terms of instantiating the number of mappers, instantiating the number of reducers, The data movement between the mappers and the reducers, all of those chores, are taken care of by the programming environment. All that the domain expert had to do was to write the map and reduce function that is unique to particular specifics of his application. The rest of the heavy lifting is all done by the programming framework.
</li> 
</ul>

<h2>4. Heavy Lifting Done by the Runtime</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/18.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Now let's look at all the heavy lifting that needs to be done by runtime in order to facilitate this map-produced style of programming. The app developer writes his map function and reduce function, and instantiates the programming library by calling map produce. And the programming library splits the input files that is provided by the user, that is a key value space provided by the user, into M splits. The number of splits of the input key value pairs,namely M, can be automatic by the programming system. Or M can also be specified by the user. In any case, once M is specified or automatically determined by the programming framework, it splits the input key value space into M splits. The next thing that happens in the programming environment is to spawn the master for this particular map reduced computation, and all the worker threads That are going to carry out the work involved in map functions and reduce functions. The master is special because the master is the one that is sort of overseeing the whole operation and keeping tab on which workers are doing what functions, when are they done,when to start new work, when to say that the computation is done. All of those chores are being orchestrated by this master. The next thing that happens is that the master is going to assign some number of worker threads as the mapper worker threads and the number of mapper working threads may correspond to the number of splits that it has done in the beginning. So there are M splits, then M worker threads are going to be assigned to mapping function. So each worker thread is going to take one portion of the input file split that have been done, and apply the map function on that particular input split. The next thing that happens is that the master assigns reduce tasks to some number of worker threads and the number of reducers to be assigned to the workers, R, is something that is decided by the application. Recall in the example that I gave you about looking for specific names. In an input corpus, the number of unique names is something that the app developer is specifying. That's where the number R comes from. That's the number of splits that the user is specifying, and that parameter is going to be used by the master to assign some number of workers as reducers. The next thing that the master does. Is to plumb the mapper to the reducers. Because now when the mappers produce that output. That output has to be sent over to the consumers of the intermediate results of the mapper, namely the reducers. And setting up this communication path between the producers of data The mappers and the consumers of data that it uses is the plumbing that the master does as the next thing. Now it's time to get to work. The map phase, what it is going to do is, it is going to read its respective split. So each of these workers is assigned to the mapping function. So each of the worker is working on a particular split. And what they're going to do is read from the local disc, the split that they've been assigned, parse the input, and then call the user defined map function. The user defined map function is the one that is doing the core of the work that needs to be done on the input data set to produce the intermediate output. The rest of it are things that needs to be done in order to facilitate the work to be carried out by the domain expert in the map function. And the intermediate key value pairs that we produced by the mapper will be buffered in memory, so each one of these workers Is doing a portion of processing the input key value place and producing the respective outputs. And periodically the intermediate results are going to be written to files, which are on the local disks of the worker or the respective workers. For this guy, on its local disk, its going to write intermediate files corresponding to the output of the map function. Similarly this worker is going to write to its local disk, the intermediate files and so on. And because the application developer has specified that there are R splits in the reducers. Each worker, meaning each map function that is associated with that worker, is going to produce R intermediate file. One for each of the R workers that are going to be doing the reduce operation. And all these intermediate files are stored on the local disk associated with each of these computational nodes carrying out the worker function corresponding to the map phase. And when they are done with the map operation for the split that they are handling, the worker will inform the master that i'm done. And what the master's waiting on is waiting on all of these mappers to get done before letting the workers to get going on the input data set. So in this sense, the master is like the conductor of an orchestra. He is started this mapping function waiting for all of these guys to get done, and when they indicate that they have done the work by notifying the master. And all of the M mappers that have been assinged to these workers have completed that work, then the masters say, okay now it is time to move on to the reduced phase.
</li> 
</ul>

<h2>5. Heavy Lifting Done by the Runtime (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/19.JPG?raw=true" alt="drawing" width="700"/>
</p>


<ul>
  <li>In the reduce phase, there is more work to be done in pulling the data that is needed for each one of these reducers. Because, we know that the mappers have been executing on different nodes of the computational cluster, and they produce their intermediate results as files on their local disk. And this worker that is carrying out a particular split of the reduce operation has to reach out and pull the data from all of the m mappers that have stored their intermediate results on their respective local disks. So there is remote read that is involved. As the first thing in the reduce phase is to pull the data. This is part of what I mean by the plumbing that the runtime system provides is to recognize that, for this reduce operation to work, it needs the mapping results from all the m nodes that carried out the map function. And so it is going to do RPC in order to get all this data from all the local disks of the nodes on which the map was executed. And once it has all the data, it can sort it, and then call the reduce function. The reduce function is the one that has been written by the domain expert. And this is the point at which the domain expertise comes in, in saying, well, I've got the data now, let me do the processing that I want to do for the reduce operation. The sorting that is being done as part of the programming framework may be to sort the input that is coming in from all of these different mappers, so that all the same keys are together in the input data set that is going to be given to the reduce function. And once such sorting has been done, the programming framework will call the user-supplied reduce function for each key with the set of intermediate values so that the reduce function can do its thing, which is domain specific. Each reduce function will then write to the final output file specific to the partition that it is dealing with. So, if you think about the original example that we started with, if, let's say, this guy is accumulating all the instances of the name Kishore, then it will write the output file that says oh, I've found so many instances of the name Kishore in the input corpus of data. And similarly, this guy may be doing it for another name like Drew, or Arun, and so on. And once each worker has completed its work by writing its final output file for this partition that it is responsible for, then it informs the master, that, yes, I'm done with the work that was assigned to me. The user program can be woken up when all the reducers have indicated to the master that they have done the work, and at that point the map reduce computation that was initiated by the user program is complete. We said that there could be m splits of the input dataset, meaning the input key-value pairs, and there could be R splits of the output that has to be generated by the application as a whole. Now, the computational resources that are available in the data center, N, may be less than the sum m plus R. So there may not be a unique machine to assign for each one of the m splits that have been made. It is the responsibility of the master to manage the machines and assign the machines that are available to handing the m input data sets as well as the R reduce splits that need to be generated. So this is part of the heavy lifting that has to be done by the runtime. So, for instance, let's say that I have only 100 worker nodes available as mappers. And I have an input split of 1000, then what I'm going to do is, I'm going to assign one split to this worker. And when the guy says I'm done with it, then I'd say, oh, you're done? Okay, take the next split and work on it. Take the next split and work on it. So that's how we're going to manage the available resources for carrying out the work that needs to be done. So remember that this is all done as part of the heavy lifting by the runtime, the user doesn't have to worry about it. All that the user did was write the map function and write the reduce function, and the rest of it is magic so far as the map reduce framework is concerned. And similarly, the number of R splits specified by the user may be more than the number of workers that are available to carry that out. And in that case, again, the master is going to assign a particular split to this worker so that he can compute and generate the output file corresponding to that split. Once he's done with that and notifies the master, then the master will say, okay, now that you're done with that, work on the next split. And it'll do all the work that is associated with plumbing, meaning, bringing the data for the split that a particular worker is working on right now, sorting it to put all the corresponding keys together, and then finally calling the reduce function that has been written by the user. That's the kind of work that goes on under the covers, in the programming framework of map reduce.
</li> 
</ul>

<h2>6. Issues to be handled by the Runtime</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l9/20.JPG?raw=true" alt="drawing" width="700"/>
</p>

<ul>
  <li>Lots of work to be done by the runtime system, to manage and map_reduce computation. The master data structures include the location of files created by the completed mappers. Remember that each mapper is working on some node of the computational cluster. Producing its output as files on its local disk. So the master has to have knowledge of where those files reside. Created by those mappers, and the namespace of the files that have been created by the mappers. And it also has to keep a score board, of the mappers and reducers that are currently assigned to work on different splits. Because the number of machines that may be available, may be less than the total number of machines that will be needed. If I wanted to do all of the input m splits in parallel and all of the R output splits in parallel. And therefore, the scoreboard is saying, at any point in time, who are all the workers carrying all the mapper functions? Who are all the workers carrying all the reducer functions? When are they done? And when they are done. How should I reassign that worker to a new task. So these are the kind of things that the master data structures facilitate the master to do. Then the big thing is fault tolerance. For a variety of reasons, the master may find, that an instance, of a map function that it started, is not responding in a timely manner. Maybe that node is down for some reason, maybe the link is down for some reason, or maybe that processor is taking a little more time than what the master expects. It can happen very easily, because in a. Data center environment where you have thousands of machines, there could be some machines that are slower than other machines, there could be generational differences between the machines that populate the data center, even though they may be homogenous in terms of the capabilities. Programming alignments and even the machine architecture. They may have speeds that are different because they correspond to different generations of processes. In any event, what might happen is that a master may notice that there is no timely response from a particular instance. Let's say of a mapper. In that case, it might say well I'm going to go ahead assume that that mapper is dead. I'm going to restart that mapping function on a different node of the cluster. Now, when that happens it is possible that the original mapper is actually dead, or it could be that it was just slow. In that case, you could get completion message from more than one mapper for the same split. So when you get multiple completion messages from redundant stragglers, the master has to be able to filter the mordancy. This is something that is redundant work. I don't have to care about that. So that's part of fault tolerance that the master has to worry about. Locality management is another important thing. In the memory hierarchy, making sure that the working set of computations fit in the closest level of the memory hierarchy of a process is very important, and this is another thing that has to be managed very carefully so that the computation can make good forward progress. In completing the map produce function. There is an inherent assumption in the fault tolerance model of the map produce framework. And that is item potency of the mapper. That is, even though the same input data split is being worked on by multiple mappers, It doesn't affect the semantics of the computation as a whole, and that is the reason why the master can simply ignore the redundant stragglers' message if in fact it was a slow computation engine that was working on a particular map function. And if it finishes later than when it was supposed to, and the master has already started another redundant computation to take care of that particular split, ignoring that redundant straggler message. Is okay because of the item potency assumption about the mapping operation. In a similar manner, the master may assign a new node to carry out reduced function corresponding to a particular split if there's no timely response from one of the reduced workers. And here again, the output file generation that is associated with a particular. A reducer split depends on the atomicity of the rename system call that is used to finally say that oh, this is the old profile generated, and the master says okay, I'm going to commit this as the real output file of the computation. Each reducer is writing a local file, and finally when the master gets the notification from the reducer, that it has completed its work, at that point, master renames that local file as the final output file, and this is where can get rid of redundant stranglers who come along later and say I produce the same output file for the same split. Master will say, I already got it, I don't need this, and it can ignore the completion message from that redundant straggler. The other thing that the programming environment has to worry about is locality management, and this is accomplished using the Google file system that provides a way by which, efficiently, data can be. Migrated from the mappers to the reducers. Task granularity is another important issue. As I mentioned, the number of nodes available in the computation cluster may be less than the sum M plus R, where M is the number of input splits, and R is the number of. Reducer splits. And it is the responsibility of the programming framework to come up with the right task granularity so that there can be good load balance of the computational resources. And the programming framework also offers several refinements to the basic model. The way the partitioning of the input data space is done, is by using a hash function that is built into the programming environment. But the user can override that partitioning function with his or her own partitioning function if they think that that'll result in a better way of organizing the data. And in the map reduced framework, there is no interaction between the mappers, but if there's some ordering guarantees needed in terms of ordering the keys and so on, that is something that the user may have to take care of. The other thing that the user may also do is combining. A partial merging to increase the granularity of the tasks that execute a mapping function. Remember that when we looked at the simple example of looking for specific names and input corpus of data, I mentioned that. The mapper can be as simple as emitting a one for a value every time it encounters the name Kishore for instance in an input file. Or, it could take an input file and count all the occurrences of Kishore in that input file and finally emit the total value as the output of the mapper. That kind of combining function is something that. A user may choose to incorporate in writing his mapping and reduce functions. It is very important to recognize that in order for the fault tolerance model of the programming environment to work correctly, map and reduce functions have to be item potent. That's a fundamental assumption. That the fault tolerant monitoring of the programming framework is based on. And there are also other bells and whistles in the programming framework for getting status information and logs and so on and so forth, but the basic idea I want to leave you with is the fact that the programming framework is a very simple one. It has two operations, map and reduce and using those two operations. You can specify a complex application that you want to call up, to run on a data center, using the computation resources that's available in the data center.
</li> 
</ul>

<h2>7. MapReduce Conclusion</h2>
<ul>
  <li>The power of Map Produce is it's simplicity. The domain expert has to write just the Map and reduce functions for his application. All the heavy lifting is completely managed by the run time system under the covers.
</li> 
</ul>

# L09c: Content Delivery Networks
<h2>1. Content Delivery Networks Introduction</h2>

<p align="center">
   <img src="https://www.nicepng.com/png/detail/11-117393_to-be-continued-meme-png-street-sign.png" alt="drawing" width="500"/>
</p>

<!-- <ul>
  <li></li> 
  <li></li> 
  <li></li> 

</ul> -->
<!-- <h2></h2>

<p align="center">
   <img src="" alt="drawing" width="500"/>
</p>

<ul>
  <li></li> 
  <li></li> 
  <li></li> 

</ul> -->
